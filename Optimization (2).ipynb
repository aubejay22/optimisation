{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwyDjRk0w4df"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import rosen, rosen_der, rosen_hess\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette section présente plusieurs fonctions test couramment utilisées pour évaluer les algorithmes d'optimisation. Chaque fonction est accompagnée de son gradient et de sa matrice Hessienne, permettant ainsi de tester différents aspects des algorithmes d'optimisation. Les fonctions test incluent la fonction Sphere, Booth, Matyas et Rosenbrock.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   **La fonction Sphere est définie par :**\n",
        "\n",
        "\\begin{equation}\n",
        "    f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2\\,, \\quad \\mathbf{x} \\in \\mathbb{R}^n\n",
        "\\end{equation}\n",
        "\n",
        "*  *Maximum global :* $f(x_1, \\ldots, x_n) = f(0, \\ldots, 0) = 0$\n",
        "*  *Objectif :* Tester la capacité des algorithmes à converger vers l'origine et minimiser une fonction quadratique simple.\n",
        "\n",
        "\n",
        "\n",
        "2.   **La fonction Booth est définie par :**\n",
        "\n",
        "\\begin{equation}\n",
        "    f(\\mathbf{x}) = (x_1 + 2 x_2 - 7)^2 + (2 x_1 + x_2 - 5)^2\\,, \\quad \\mathbf{x} \\in \\mathbb{R}^2\n",
        "\\end{equation}\n",
        "*  *Maximum global :* $f(x_1, x_2) = f(1, 3) = 0 $.\n",
        "*  *Objectif :* Évaluer la capacité des algorithmes à gérer des fonctions quadratiques non convexes avec des points de minimum bien définis.\n",
        "\n",
        "3.   **La fonction Matyas est définie par :**\n",
        "\n",
        "\\begin{equation}\n",
        "    f(\\mathbf{x}) = 0.26 (x_1^2 + x_2^2) - 0.48 x_1 x_2\\,, \\quad \\mathbf{x} \\in \\mathbb{R}^2\n",
        "\\end{equation}\n",
        "\n",
        "*  *Maximum global :* $f(x_1, x_2) = f(0, 0) = 0 $.\n",
        "*  *Objectif :* Tester les algorithmes d'optimisation sur une fonction quadratique avec des interactions entre les variables.\n",
        "\n",
        "\n",
        "\n",
        "4.   **La fonction Rosenbrock est définie par :**\n",
        "\n",
        "\\begin{equation}\n",
        "    f(\\mathbf{x}) = \\sum_{i=1}^{n-1} [100 (x_{i+1} - x_i^2)^2 + (x_i - 1)^2]\\,, \\quad \\mathbf{x} \\in \\mathbb{R}^n\n",
        "\\end{equation}\n",
        "*  *Maximum global :* $f(x_1,\\ldots, x_n) = f(1,\\ldots, 1) = 0 $.\n",
        "*  *Objectif :*  Évaluer les algorithmes d'optimisation sur une fonction complexe avec des interactions non linéaires entre les variables.\n",
        "\n",
        "Les fonctions définies ici permettent de tester et d'évaluer les performances des algorithmes d'optimisation. Elles aident à vérifier la convergence des algorithmes vers les minima globaux et à évaluer leur capacité à gérer différentes structures de fonctions, telles que des fonctions quadratiques, des fonctions avec interactions entre variables, et des fonctions non linéaires complexes."
      ],
      "metadata": {
        "id": "OOTL0BXdOYyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z27elwG1kXeE"
      },
      "outputs": [],
      "source": [
        "#coder les fonctions test\n",
        "\n",
        "class Sphere:\n",
        "  def __init__(self):\n",
        "    self.f = lambda x : sum([xi**2 for xi in x])\n",
        "    self.grad = lambda x : np.array([2*xi for xi in x])\n",
        "    self.hessian = lambda x: np.diag([2 for _ in range(len(x))])\n",
        "    print('le min est: [0, ..., 0] et f(x) = 0')\n",
        "\n",
        "class Booth:\n",
        "  def __init__(self):\n",
        "    self.f = lambda x : (x[0] + 2*x[1] - 7)**2 + (2*x[0] + x[1] - 5)**2\n",
        "    #self.f = lambda x, y : (x + 2*y - 7)**2 + (2*x + y - 5)**2\n",
        "    self.grad = lambda x : np.array([10*x[0] + 8*x[1] - 34, 8*x[0] + 10*x[1] - 38])\n",
        "    #self.grad = lambda x, y : np.array([10*x + 8*y - 34, 8*x + 10*xy - 38])\n",
        "    self.hessian = lambda x: np.array([[10, 8], [8, 10]])\n",
        "    print('le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10')\n",
        "\n",
        "\n",
        "class Matyas:\n",
        "  def __init__(self):\n",
        "    self.f = lambda x : 0.26*(x[0]**2 + x[1]**2) - 0.48*x[0]*x[1]\n",
        "    self.grad = lambda x : np.array([0.52*x[0] - 0.48*x[1], 0.52*x[1] - 0.48*x[0]])\n",
        "    self.hessian = lambda x: np.array([[0.52, -0.48], [-0.48, 0.52]])\n",
        "    print('le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10')\n",
        "\n",
        "\n",
        "class Rosenbrock:\n",
        "  def __init__(self):\n",
        "    #self.f = lambda x : np.sum([(1 - x[i])**2 + 100 * (x[i+1] - x[i]**2)**2 for i in range(len(x)-1)])\n",
        "    self.f = lambda x : rosen(x)\n",
        "    self.grad = lambda x : rosen_der(x)\n",
        "    self.hessian = lambda x : rosen_hess(x)\n",
        "    print('le min est: [1, ...,  1] et f(1, ...,  1) = 0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode de line search (recherche le long d'une ligne) est une stratégie d'optimisation qui consiste à choisir une direction $\\mathbf{p}_k$ à partir de l'itération actuelle $\\mathbf{x}_k$ et à rechercher un nouvel itéré ayant une valeur de fonction plus basse le long de cette direction. On définit alors un problème de minimisation unidimensionnel pour trouver la longueur de pas $\\alpha$ :\n",
        "\n",
        "\\begin{equation}\n",
        "\\min_{\\alpha > 0} f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "En résolvant (1) de manière exacte, on tirerait le maximum de bénéfice de la direction $\\mathbf{p}_k$. Toutefois, une minimisation exacte peut être coûteuse et généralement inutile. Ainsi, l'algorithme de line search génère un nombre limité de longueurs de pas d'essai jusqu'à en trouver une qui approche de manière lâche le minimum de (1). À ce nouveau point $\\mathbf{x}_{k+1}$, une nouvelle direction de recherche et une nouvelle longueur de pas sont calculées, et le processus est répété.\n",
        "\n",
        "## Algorithmes de Line Search\n",
        "\n",
        "1.  *Gradient Descendant:*\n",
        "Le gradient descendant est basé sur la direction du gradient de la fonction $f$. À chaque itération, la direction de recherche est donnée par :\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k),\n",
        "\\end{equation}\n",
        "\n",
        "$\\hspace{20pt}$où $\\nabla f(\\mathbf{x}_k)$ est le gradient de $f$ à $\\mathbf{x}_k$. Le but est de minimiser $f$ en se déplaçant dans la direction opposée au gradient, car c'est là que la $\\hspace{20pt}$fonction diminue le plus rapidement.\n",
        "\n",
        "\\\\\n",
        "\n",
        "2.  *Méthode de Newton:*\n",
        "La méthode de Newton utilise une approximation de la fonction objective par une série de Taylor d'ordre deux :\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p} + \\frac{1}{2} \\mathbf{p}^T \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}.\n",
        "\\end{equation}\n",
        "\n",
        "$\\hspace{20pt}$En minimisant cette approximation, on obtient la direction de Newton :\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{p}_k = -(\\nabla^2 f(\\mathbf{x}_k))^{-1} \\nabla f(\\mathbf{x}_k),\n",
        "\\end{equation}\n",
        "\n",
        "$\\hspace{20pt}$sous l'hypothèse que la matrice Hessienne $\\nabla^2 f(\\mathbf{x}_k)$ est définie positive.\n",
        "\n",
        "\\\\\n",
        "\n",
        "3. *Méthode BFGS:*\n",
        "La méthode BFGS (Broyden-Fletcher-Goldfarb-Shanno) est une méthode quasi-Newton qui construit une approximation de la Hessienne à partir des gradients. À chaque itération, elle met à jour une matrice $\\mathbf{B}_k$ qui approxime $\\nabla^2 f(\\mathbf{x}_k)$ sans avoir à calculer la Hessienne exacte. La direction de recherche est alors :\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f(\\mathbf{x}_k).\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "Les méthodes de line search offrent des approches efficaces pour résoudre des problèmes d'optimisation non contraints. Le choix de la direction de recherche $\\mathbf{p}_k$ et de la longueur de pas $\\alpha_k$ est crucial pour garantir la convergence rapide vers un minimum local de la fonction $f$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "## Convergence des Algorithmes:\n",
        "\n",
        "La convergence des algorithmes d'optimisation est justifiée par le théorème suivant :\n",
        "\n",
        "$\\hspace{20pt}$*Théorème:* Si $x^*$ est un minimiseur local et $f$ est continûment dérivable dans un $\\hspace{20pt}$voisinage ouvert de $x^*$, alors $\\nabla f(x^*) \\neq 0$.\n",
        "\n",
        "Ainsi, lorsque $\\|\\nabla f(\\mathbf{x}_k)\\| \\to 0$, l'algorithme s'arrête, indiquant que $\\mathbf{x}_k$ approche un minimum local."
      ],
      "metadata": {
        "id": "BibGf3TWJooe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJduf9DKdbcy"
      },
      "outputs": [],
      "source": [
        "def algo_gradient(func, grad, x_k, epsilon, step):\n",
        "    \"\"\"\n",
        "    Algorithme de gradient pour minimiser une fonction objectif.\n",
        "\n",
        "    :param func: Fonction objectif à minimiser.\n",
        "    :param grad: Fonction qui calcule le gradient de la fonction objectif.\n",
        "    :param x_k: Point de départ.\n",
        "    :param epsilon: Critère d'arrêt pour la norme du gradient.\n",
        "    :param step: Fonction lambda ou valeur constante pour déterminer la taille du pas.\n",
        "    :return: Point final après convergence.\n",
        "    \"\"\"\n",
        "    while np.linalg.norm(grad(x_k)) > epsilon:  # Critère d'arrêt\n",
        "        pk = -grad(x_k)\n",
        "\n",
        "        if callable(step):  # Si step est une fonction lambda, on l'appelle pour obtenir alpha\n",
        "            alpha = step(x_k, pk)\n",
        "        else:  # Sinon, on utilise step comme une valeur constante\n",
        "            alpha = step\n",
        "\n",
        "        # Mise à jour du point x_k\n",
        "        x_k = x_k + alpha * pk\n",
        "    x_k = [f\"{val:.16f}\" for val in x_k]\n",
        "    return x_k\n",
        "\n",
        "\n",
        "\n",
        "def newtone(func, grad, hessian, x_k, epsilon, step):\n",
        "   \"\"\"\n",
        "    Algorithme de Newton pour minimiser une fonction objectif.\n",
        "\n",
        "    :param func: Fonction objectif à minimiser.\n",
        "    :param grad: Fonction qui calcule le gradient de la fonction objectif.\n",
        "    :param hessian: Fonction qui calcule la matrice Hessienne de la fonction objectif.\n",
        "    :param x_k: Point de départ.\n",
        "    :param epsilon: Critère d'arrêt pour la norme du gradient.\n",
        "    :param step: Fonction lambda ou valeur constante pour déterminer la taille du pas.\n",
        "    :return: Point final après convergence.\n",
        "   \"\"\"\n",
        "   while np.linalg.norm(grad(x_k)) >  epsilon: #critere d'arret\n",
        "     pk = -np.linalg.inv(hessian(x_k))@grad(x_k)\n",
        "\n",
        "     if callable(step):  # Si step est une fonction lambda, on l'appelle pour obtenir alpha\n",
        "        alpha = step(x_k, pk)\n",
        "     else:  # Sinon, on utilise step comme une valeur constante\n",
        "        alpha = step\n",
        "\n",
        "     #3nouvelle itéré\n",
        "     x_k = x_k + alpha * pk\n",
        "\n",
        "   x_k = [f\"{val:.16f}\" for val in x_k]\n",
        "   return x_k\n",
        "\n",
        "\n",
        "\n",
        "def bfgs(func, grad, x_k, epsilon, step):\n",
        "   \"\"\"\n",
        "    Algorithme BFGS pour minimiser une fonction objectif.\n",
        "\n",
        "    :param func: Fonction objectif à minimiser.\n",
        "    :param grad: Fonction qui calcule le gradient de la fonction objectif.\n",
        "    :param x_k: Point de départ.\n",
        "    :param epsilon: Critère d'arrêt pour la norme du gradient.\n",
        "    :param step: Fonction lambda ou valeur constante pour déterminer la taille du pas.\n",
        "    :return: Point final après convergence.\n",
        "   \"\"\"\n",
        "   #initialisation de l'approximation de la matrice hessian\n",
        "   b_k = np.eye(len(x_k))\n",
        "\n",
        "   while np.linalg.norm(grad(x_k)) >  epsilon: #critere d'arret\n",
        "     pk = np.dot(np.linalg.inv(b_k), grad(x_k)) #vecteur directionnel\n",
        "     if callable(step):  # Si step est une fonction lambda, on l'appelle pour obtenir alpha\n",
        "        alpha = step(x_k, pk)\n",
        "     else:  # Sinon, on utilise step comme une valeur constante\n",
        "        alpha = step\n",
        "\n",
        "     #1) nouvelle itéré\n",
        "     x_kplus1 = x_k - alpha * pk\n",
        "\n",
        "     #update matrice bk\n",
        "     y_k = grad(x_kplus1) - grad(x_k)\n",
        "     s_k = x_kplus1 - x_k\n",
        "     s_k = s_k[:, np.newaxis]  # (n, 1)\n",
        "     y_k = y_k[:, np.newaxis]  # (n, 1)\n",
        "\n",
        "\n",
        "     b_k = b_k  -  (b_k @ s_k @ s_k.T @ b_k) / (s_k.T @ b_k @ s_k)  +  np.dot(y_k, y_k.T) / (y_k.T @ s_k)\n",
        "     #reinitialisation de x_k\n",
        "     x_k = x_kplus1\n",
        "\n",
        "   x_k = [f\"{val:.16f}\" for val in x_k]\n",
        "   return x_k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# algo gradient , newtone et bfgs test"
      ],
      "metadata": {
        "id": "9ldtd5SAfejo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_fUS8xtnkJO",
        "outputId": "6c7aaf19-80fc-4d5e-9e3a-1307d83c188d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "fonction sphere\n",
            "Résultat final: ['0.0000000000003553', '0.0000000000355271', '0.0000000000000107', '0.0000000000000036', '0.0000000000000178']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10\n",
            "fonction booth\n",
            "Résultat final: ['1.0000000000039173', '3.0000000000039213']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10\n",
            "fonction matyas\n",
            "Résultat final: ['0.0000000017658246', '0.0000000017658246']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, ...,  1] et f(1, ...,  1) = 0\n",
            "fonction rosenbrock\n",
            "Résultat final: ['0.9999999999542477', '0.9999999999083210', '0.9999999998162055']\n",
            "Time = 17.40215301513672\n"
          ]
        }
      ],
      "source": [
        "epsilon = 1e-10\n",
        "\n",
        "\n",
        "sphere = Sphere()\n",
        "x_0 = np.array([100, 10000, 3.0, 1, 5])\n",
        "algo_grad = algo_gradient(sphere.f, sphere.grad, x_0, epsilon, 0.25)\n",
        "print('fonction sphere')\n",
        "print(\"Résultat final:\", algo_grad)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "x_0 = np.array([2, 4])\n",
        "booth = Booth()\n",
        "grad_booth = algo_gradient(booth.f, booth.grad, x_0, epsilon, 0.001)\n",
        "print('fonction booth')\n",
        "print(\"Résultat final:\", grad_booth)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "matyas = Matyas()\n",
        "grad_matyas = algo_gradient(matyas.f, matyas.grad, x_0, epsilon,0.05)\n",
        "print('fonction matyas')\n",
        "print(\"Résultat final:\", grad_matyas)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "x_0 = np.array([3, 3, 3])\n",
        "rosenbrock = Rosenbrock()\n",
        "t0 = time.time()\n",
        "min = algo_gradient(rosenbrock.f, rosenbrock.grad, x_0, epsilon, 0.00055)\n",
        "t1 = time.time()\n",
        "print('fonction rosenbrock')\n",
        "print(\"Résultat final:\", min)\n",
        "print(f\"Time = {t1 - t0}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sphere = Sphere()\n",
        "x_0 = np.array([1.0, 2.0, 3.0, 1, 5])\n",
        "min = newtone(sphere.f, sphere.grad, sphere.hessian, x_0, epsilon, 1)\n",
        "print('fonction sphere')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "x_0 = np.array([2, 4])\n",
        "booth = Booth()\n",
        "min = newtone(booth.f, booth.grad, booth.hessian, x_0, epsilon, 1)\n",
        "print('fonction booth')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "matyas = Matyas()\n",
        "min = newtone(matyas.f, matyas.grad, matyas.hessian, x_0, epsilon, 1)\n",
        "print('fonction matyas')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "x_0 = np.array([2, 3])\n",
        "rosenbrock = Rosenbrock()\n",
        "min = newtone(rosenbrock.f, rosenbrock.grad, rosenbrock.hessian, x_0, epsilon, 1)\n",
        "print('fonction rosenbrock')\n",
        "print(\"Résultat final:\", min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-FHJyS6Ch-B",
        "outputId": "f8194d67-e39f-4527-f99a-d86f46373a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "fonction sphere\n",
            "Résultat final: ['0.0000000000000000', '0.0000000000000000', '0.0000000000000000', '0.0000000000000000', '0.0000000000000000']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10\n",
            "fonction booth\n",
            "Résultat final: ['0.9999999999999998', '3.0000000000000000']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10\n",
            "fonction matyas\n",
            "Résultat final: ['0.0000000000000016', '0.0000000000000022']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, ...,  1] et f(1, ...,  1) = 0\n",
            "fonction rosenbrock\n",
            "Résultat final: ['1.0000000000028562', '1.0000000000057121']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sphere = Sphere()\n",
        "x_0 = np.array([1.0, 2.0, 3.0, 1, 5])\n",
        "min = bfgs(sphere.f, sphere.grad, x_0, epsilon, 1)\n",
        "print('fonction sphere')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "x_0 = np.array([2, 4])\n",
        "booth = Booth()\n",
        "min = bfgs(booth.f, booth.grad, x_0, epsilon, 1)\n",
        "print('fonction booth')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "matyas = Matyas()\n",
        "min = bfgs(matyas.f, matyas.grad, x_0, epsilon, 1)\n",
        "print('fonction matyas')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "\n",
        "x_0 = np.array([2, 3])\n",
        "rosenbrock = Rosenbrock()\n",
        "min = bfgs(rosenbrock.f, rosenbrock.grad, x_0, epsilon, 1)\n",
        "print('fonction rosenbrock')\n",
        "print(\"Résultat final:\", min)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsxrjpKFgh8J",
        "outputId": "6365252e-cda3-4f2d-80c3-5f62f45b77e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "fonction sphere\n",
            "Résultat final: ['0.0000000000000002', '0.0000000000000004', '-0.0000000000000004', '0.0000000000000000', '0.0000000000000000']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10\n",
            "fonction booth\n",
            "Résultat final: ['0.9999999999999858', '3.0000000000000142']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10\n",
            "fonction matyas\n",
            "Résultat final: ['0.0000000000008312', '0.0000000000011101']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, ...,  1] et f(1, ...,  1) = 0\n",
            "fonction rosenbrock\n",
            "Résultat final: ['0.9999999999997330', '0.9999999999994632']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUyq65sompBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conditions de wolfe\n"
      ],
      "metadata": {
        "id": "SaS78X5YOU1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wolf:\n",
        "    def __init__(self, f, grad, alpha_max=1.0, c1=1e-4, c2=0.9):\n",
        "        \"\"\"\n",
        "        Initialisation de la classe pour la recherche de ligne avec rétrogradation utilisant les conditions de Wolfe.\n",
        "\n",
        "        :param f: Fonction objectif à minimiser.\n",
        "        :param grad: Fonction qui calcule le gradient de la fonction objectif.\n",
        "        :param alpha_max: Valeur initiale de α.\n",
        "        :param c1: Paramètre pour la condition d'Armijo (condition de suffisance de la diminution).\n",
        "        :param c2: Paramètre pour la condition de Wolfe (condition de courbure).\n",
        "        \"\"\"\n",
        "        self.f = f\n",
        "        self.grad = grad\n",
        "        self.alpha_max = alpha_max\n",
        "        self.c1 = c1\n",
        "        self.c2 = c2\n",
        "\n",
        "        if (c2 < c1) or (c1 <= 0) or (alpha_max <= 0):\n",
        "            raise ValueError('Les conditions 0 < c1 < c2 < 1 ne sont pas respectées ou alpha_max est inférieur ou égal à zéro.')\n",
        "\n",
        "    def backtracking(self, xk, pk, rho=0.5):\n",
        "        \"\"\"\n",
        "        Calcule la taille du pas α en utilisant la méthode de recherche de ligne avec rétrogradation.\n",
        "\n",
        "        :param xk: Point actuel.\n",
        "        :param pk: Direction de descente.\n",
        "        :param rho: Facteur de réduction pour la rétrogradation.\n",
        "        :return: Taille du pas α.\n",
        "        \"\"\"\n",
        "        alpha = self.alpha_max\n",
        "        while self.f(xk + alpha * pk) > self.f(xk) + self.c1 * alpha * np.dot(self.grad(xk), pk):\n",
        "            alpha = alpha * rho\n",
        "        return alpha\n",
        "\n",
        "    def interpolation(self, xk, pk):\n",
        "        \"\"\"\n",
        "        Calcule la taille du pas α en utilisant la méthode d'interpolation pour satisfaire les conditions de Wolfe.\n",
        "\n",
        "        :param xk: Point actuel.\n",
        "        :param pk: Direction de descente.\n",
        "        :return: Taille du pas α.\n",
        "        \"\"\"\n",
        "        if (self.c2 < self.c1) or (self.c1 <= 0) or (self.alpha_max <= 0):\n",
        "            raise ValueError('Les conditions 0 < c1 < c2 < 1 ne sont pas respectées ou alpha_max est inférieur ou égal à zéro.')\n",
        "\n",
        "        # Suite de α\n",
        "        alpha = np.linspace(1e-16, self.alpha_max, 10)\n",
        "\n",
        "        # Définition de la fonction phi(α) et sa dérivée\n",
        "        phi = lambda alpha: self.f(xk + alpha * pk)\n",
        "        d_phi = lambda alpha: np.dot(self.grad(xk + alpha * pk), pk)\n",
        "\n",
        "        phi_0 = phi(0)\n",
        "        d_phi_0 = d_phi(0)\n",
        "\n",
        "        def zoom(alpha_lo, alpha_hi):\n",
        "            \"\"\"Recherche dans un intervalle plus petit en satisfaisant les conditions de Wolfe.\"\"\"\n",
        "            while True:\n",
        "                alpha_j = (alpha_hi + alpha_lo) / 2\n",
        "                if (phi(alpha_j) > phi_0 + self.c1 * alpha_j * d_phi_0) or (phi(alpha_j) >= phi(alpha_lo)):\n",
        "                    alpha_hi = alpha_j\n",
        "                    if abs(alpha_hi - alpha_lo) < 1e-8:\n",
        "                        return (alpha_hi + alpha_lo) / 2\n",
        "                else:\n",
        "                    d_phi_j = d_phi(alpha_j)\n",
        "                    if abs(d_phi_j) <= -self.c2 * d_phi_0:\n",
        "                        return alpha_j\n",
        "                    if d_phi_j * (alpha_hi - alpha_lo) >= 0:\n",
        "                        alpha_hi = alpha_lo\n",
        "                    alpha_lo = alpha_j\n",
        "\n",
        "        # Boucle pour vérifier si alpha respecte les conditions de Wolfe\n",
        "        for i in range(len(alpha)):\n",
        "            if (phi(alpha[i]) > phi_0 + self.c1 * alpha[i] * d_phi_0) or (i > 1 and phi(alpha[i]) >= phi(alpha[i - 1])):\n",
        "                return zoom(alpha[i - 1], alpha[i])\n",
        "            if abs(d_phi(alpha[i])) <= -self.c2 * d_phi_0:\n",
        "                return alpha[i]\n",
        "            if d_phi(alpha[i]) >= 0:\n",
        "                return zoom(alpha[i], alpha[i - 1])\n",
        "\n",
        "        return 1  # Si aucun autre alpha n'est trouvé, on retourne 1"
      ],
      "metadata": {
        "id": "NtD4wNqln4dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algo backtracking"
      ],
      "metadata": {
        "id": "4aMdkfsQn1xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epsilon = 1e-10\n",
        "\n",
        "# Fonction Sphere\n",
        "sphere = Sphere()\n",
        "x_0 = np.array([100, 100, 100.0, 100, 100])\n",
        "wolf_sphere = Wolf(sphere.f, sphere.grad, alpha_max=1.0, c1=1e-4, c2=0.9)\n",
        "step_sphere = lambda xk, pk: wolf_sphere.backtracking(xk, pk)\n",
        "algo_grad_sphere = algo_gradient(sphere.f, sphere.grad, x_0, epsilon, step_sphere)\n",
        "print('Fonction Sphere')\n",
        "print(\"Résultat final:\", algo_grad_sphere)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Sphere\n",
        "sphere = Sphere()\n",
        "x_0 = np.array([100, 100, 100.0, 100, 100])\n",
        "wolf_sphere = Wolf(sphere.f, sphere.grad, alpha_max=1.0, c1=1e-4, c2=0.9)\n",
        "step_sphere = lambda xk, pk: wolf_sphere.backtracking(xk, pk)\n",
        "algo_grad_sphere = newtone(sphere.f, sphere.grad, sphere.hessian, x_0, epsilon, step_sphere)\n",
        "print('Fonction Sphere')\n",
        "print(\"Résultat final:\", algo_grad_sphere)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Booth\n",
        "x_0 = np.array([2, 4])\n",
        "booth = Booth()\n",
        "wolf_booth = Wolf(booth.f, booth.grad, alpha_max=1.0, c1=1e-4, c2=0.9)\n",
        "step_booth = lambda xk, pk: wolf_booth.backtracking(xk, pk)\n",
        "grad_booth = algo_gradient(booth.f, booth.grad, x_0, epsilon, step_booth)\n",
        "print('Fonction Booth')\n",
        "print(\"Résultat final:\", grad_booth)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Matyas\n",
        "x_0 = np.array([2, 3])\n",
        "matyas = Matyas()\n",
        "wolf_matyas = Wolf(matyas.f, matyas.grad, alpha_max=1, c1=1e-4, c2=0.9)\n",
        "step_matyas = lambda xk, pk: wolf_matyas.backtracking(xk, pk)\n",
        "grad_matyas = algo_gradient(matyas.f, matyas.grad, x_0, epsilon, step_matyas)\n",
        "print('Fonction Matyas')\n",
        "print(\"Résultat final:\", grad_matyas)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Rosenbrock\n",
        "x_0 = np.array([10, 10, 10])\n",
        "rosenbrock = Rosenbrock()\n",
        "wolf_rosenbrock = Wolf(rosenbrock.f, rosenbrock.grad, alpha_max=0.0005, c1=1e-4, c2=0.9)\n",
        "step_rosenbrock = lambda xk, pk: wolf_rosenbrock.backtracking(xk, pk)\n",
        "t0 = time.time()\n",
        "min_rosenbrock = algo_gradient(rosenbrock.f, rosenbrock.grad, x_0, epsilon, step_rosenbrock)\n",
        "t1 = time.time()\n",
        "print('Fonction Rosenbrock')\n",
        "print(\"Résultat final:\", min_rosenbrock)\n",
        "print(f\"Time = {t1 - t0}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiqD6F4yOY1V",
        "outputId": "1d821cf4-a157-4abc-f305-a179addcf2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "Fonction Sphere\n",
            "Résultat final: ['0.0000000000000000', '0.0000000000000000', '0.0000000000000000', '0.0000000000000000', '0.0000000000000000']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "Fonction Sphere\n",
            "Résultat final: ['0.0000000000000000', '0.0000000000000000', '0.0000000000000000', '0.0000000000000000', '0.0000000000000000']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10\n",
            "Fonction Booth\n",
            "Résultat final: ['0.9999999999981810', '2.9999999999981810']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10\n",
            "Fonction Matyas\n",
            "Résultat final: ['0.0000000017067405', '0.0000000017067405']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, ...,  1] et f(1, ...,  1) = 0\n",
            "Fonction Rosenbrock\n",
            "Résultat final: ['0.9999999999542472', '0.9999999999083199', '0.9999999998162032']\n",
            "Time = 15.012542486190796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditions de wolf par interpolation"
      ],
      "metadata": {
        "id": "YNvAQj1vnmUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1e-10\n",
        "\n",
        "\n",
        "\n",
        "# Fonction Sphere\n",
        "sphere = Sphere()\n",
        "x_0 = np.array([3, 2.0, 3.0, 1, 5])\n",
        "step_sphere = lambda xk, pk: wolf_sphere.interpolation(xk, pk)\n",
        "algo_grad_sphere = newtone(sphere.f, sphere.grad, sphere.hessian, x_0, epsilon, step_sphere)\n",
        "print('Fonction Sphere')\n",
        "print(\"Résultat final:\", algo_grad_sphere)\n",
        "print('\\n \\n ')\n",
        "\n",
        "\n",
        "# Fonction Booth\n",
        "x_0 = np.array([2, 4])\n",
        "booth = Booth()\n",
        "step_booth = lambda xk, pk: wolf_booth.interpolation(xk, pk)\n",
        "grad_booth = algo_gradient(booth.f, booth.grad, x_0, epsilon, step_booth)\n",
        "print('Fonction Booth')\n",
        "print(\"Résultat final:\", grad_booth)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Matyas\n",
        "x_0 = np.array([2, 3])\n",
        "matyas = Matyas()\n",
        "step_matyas = lambda xk, pk: wolf_matyas.interpolation(xk, pk)\n",
        "grad_matyas = algo_gradient(matyas.f, matyas.grad, x_0, epsilon, step_matyas)\n",
        "print('Fonction Matyas')\n",
        "print(\"Résultat final:\", grad_matyas)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Rosenbrock\n",
        "x_0 = np.array([3, 3, 3])\n",
        "rosenbrock = Rosenbrock()\n",
        "step_rosenbrock = lambda xk, pk: wolf_rosenbrock.interpolation(xk, pk)\n",
        "t0 = time.time()\n",
        "min_rosenbrock = algo_gradient(rosenbrock.f, rosenbrock.grad, x_0, epsilon, step_rosenbrock)\n",
        "t1 = time.time()\n",
        "print('Fonction Rosenbrock')\n",
        "print(\"Résultat final:\", min_rosenbrock)\n",
        "print(f\"Time = {t1 - t0}\")\n"
      ],
      "metadata": {
        "id": "RdZYUTO03LyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186811b4-c214-417c-8b47-df5748bffa8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "Fonction Sphere\n",
            "Résultat final: ['0.0000000000211773', '0.0000000000141182', '0.0000000000211773', '0.0000000000070591', '0.0000000000352956']\n",
            "\n",
            " \n",
            " \n",
            "le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10\n",
            "Fonction Booth\n",
            "Résultat final: ['0.9999999999999982', '2.9999999999999982']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10\n",
            "Fonction Matyas\n",
            "Résultat final: ['0.0000000017647207', '0.0000000017647207']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, ...,  1] et f(1, ...,  1) = 0\n",
            "Fonction Rosenbrock\n",
            "Résultat final: ['1.0000000000256468', '1.0000000000512284', '1.0000000001025378']\n",
            "Time = 1.2098114490509033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Derivative-Free Optimization algo"
      ],
      "metadata": {
        "id": "Ak91EktIgMK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'algorithme optimise une fonction $f(x)$ sans nécessiter de dérivées. À chaque itération, il évalue $f$ dans plusieurs directions $p \\in D_k$ autour du point courant $x_k$, en utilisant un pas $\\gamma_k$.\n",
        "\n",
        "\n",
        "1.   *Initialisation:*\n",
        "*   Choisir $x_0$, $\\gamma_0$, et un ensemble de directions $D_0$.\n",
        "\n",
        "\n",
        "2.   *Évaluation des points candidats:*\n",
        "*    Pour chaque $p \\in D_k$, calculer $x_k + \\gamma_k p$.\n",
        "*     Si $f(x_k + \\gamma_k p) < f(x_k) - \\rho(\\gamma_k)$, adopter  $x_{k+1} = x_k + \\gamma_k p$ et éventuellement augmenter $\\gamma_k$.\n",
        "\n",
        "\n",
        "3.   *Réduction du pas:*\n",
        "*    Si aucune direction ne diminue suffisamment $f$, réduire $\\gamma_k$ en le multipliant par $\\theta_k$ avec $0 < \\theta_k < 1$.\n",
        "\n",
        "\n",
        "4.   *Arrêt:*\n",
        "*    L'algorithme s'arrête lorsque $\\gamma_k < \\gamma_{\\text{tol}}$.\n",
        "\n",
        "\n",
        "**Conditions pour la validité:**\n",
        "\n",
        "*    *Direction de descente:* Il existe $p \\in D_k$ tel que\n",
        "\\begin{equation}\n",
        "\\cos\\theta = \\frac{- \\nabla f_k^T p}{\\|\\nabla f_k\\|\\|p\\|} > \\delta.\n",
        "\\end{equation}\n",
        "\n",
        "*    *Longueur des directions:*\n",
        "\\begin{equation}\n",
        "\\exists \\, \\beta_{\\text{min}}, \\beta_{\\text{max}} > 0, \\quad\n",
        "\\beta_{\\text{min}} \\leq \\|p\\| \\leq \\beta_{\\text{max}}, \\quad \\forall p \\in D_k.\n",
        "\\end{equation}\n",
        "\n",
        "*    *Diminution suffisante:*\n",
        "\\begin{equation}\n",
        "f(x_k + \\gamma_k p) < f(x_k) - \\rho(\\gamma_k), \\quad avec \\quad \\frac{\\rho(t)}{t} \\to 0 \\quad \\text{lorsque} \\quad t \\to 0\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{\\rho(t)}{t} \\to 0 \\quad \\text{lorsque} \\quad t \\to 0.\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "UMy_5-44JGwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pattern search\n",
        "def set(n):\n",
        "  D = np.vstack([np.eye(n), -np.eye(n)])\n",
        "  return D\n",
        "\n",
        "\n",
        "def pattern_search(f, x_0, nombre_iteration):\n",
        "  \"\"\"\n",
        "    Algorithme de recherche par patron pour minimiser une fonction sans dérivées.\n",
        "\n",
        "    :param f: Fonction objectif à minimiser.\n",
        "    :param x_0: Point de départ pour la recherche.\n",
        "    :param nombre_iteration: Nombre maximum d'itérations à effectuer.\n",
        "    :return: Point final après convergence, formaté avec 16 décimales.\n",
        "  \"\"\"\n",
        "  gamma_min = 1e-8\n",
        "  theta_max = 0.5\n",
        "  rho = lambda t : 1e-4 * t**(3/2)\n",
        "\n",
        "  gamma = 1\n",
        "  D = set(len(x_0))\n",
        "  iteration = 0\n",
        "  while (gamma > gamma_min) and (iteration < nombre_iteration):\n",
        "    xk = False\n",
        "    for pk in D:\n",
        "\n",
        "      if (f(x_0 - gamma*pk) < (f(x_0) - rho(gamma))):\n",
        "        xk = True\n",
        "        x_0 = x_0 - gamma*pk\n",
        "        gamma *= 1.1\n",
        "\n",
        "    if not xk:\n",
        "      gamma *= theta_max\n",
        "    iteration += 1\n",
        "\n",
        "  print(f\"Le nombre d'itération : {iteration}\")\n",
        "  x_0 = [f\"{val:.16f}\" for val in x_0]\n",
        "  return x_0"
      ],
      "metadata": {
        "id": "4ZMdDgV0y3sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction Sphere\n",
        "sphere = Sphere()\n",
        "x_0 = np.array([100, 100, 100.0, 100, 100])\n",
        "min = pattern_search(sphere.f, x_0, 10000)\n",
        "print('Fonction Sphere')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "\n",
        "x_0 = np.array([-2000, 1000])\n",
        "booth = Booth()\n",
        "min = pattern_search(booth.f, x_0, 10000)\n",
        "print('fonction booth')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n ')\n",
        "\n",
        "\n",
        "# Fonction Matyas\n",
        "x_0 = np.array([2, 3])\n",
        "matyas = Matyas()\n",
        "min = pattern_search(matyas.f, x_0, 10000)\n",
        "print('Fonction Matyas')\n",
        "print(\"Résultat final:\", min)\n",
        "print('\\n \\n \\n')\n",
        "\n",
        "# Fonction Rosenbrock\n",
        "x_0 = np.array([10, 10, 10])\n",
        "rosenbrock = Rosenbrock()\n",
        "min = pattern_search(rosenbrock.f, x_0, 20000)\n",
        "print('Fonction Rosenbrock')\n",
        "print(\"Résultat final:\", min)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHpkPoetlolO",
        "outputId": "12efe105-e9c4-41e4-a6e5-1291ee623efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le min est: [0, ..., 0] et f(x) = 0\n",
            "Le nombre d'itération : 108\n",
            "Fonction Sphere\n",
            "Résultat final: ['-0.0000000144830664', '0.0000000115175886', '-0.0000000101728452', '0.0000000040777529', '0.0000000023947263']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, 3] et f(1, 3) = 0             -10<= x, y <= 10\n",
            "Le nombre d'itération : 111\n",
            "fonction booth\n",
            "Résultat final: ['0.9999999830101313', '3.0000000143651491']\n",
            "\n",
            " \n",
            " \n",
            "le min est: [0, 0] et f(0, 0) = 0             -10<= x, y <= 10\n",
            "Le nombre d'itération : 123\n",
            "Fonction Matyas\n",
            "Résultat final: ['-0.0000004136703338', '-0.0000004143705779']\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "le min est: [1, ...,  1] et f(1, ...,  1) = 0\n",
            "Le nombre d'itération : 11780\n",
            "Fonction Rosenbrock\n",
            "Résultat final: ['0.9999977656881633', '0.9999955338773080', '0.9999910642263564']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzKqDTp0Tka7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# least squares"
      ],
      "metadata": {
        "id": "4pYgxl1qiCKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans les problèmes de moindres carrés, la fonction objectif $ f(x) $ est définie comme suit :\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = \\frac{1}{2} \\sum_{j=1}^{m} r_j^2(x)\n",
        "\\end{equation}\n",
        "\n",
        "où chaque $ r_j(x) $ est un résidu lisse de $ r_j : \\mathbb{R}^n \\to \\mathbb{R} $, formant ainsi un vecteur de résidus $ r(x) = (r_1(x), \\dots, r_m(x))^T $.\n",
        "\n",
        "On peut réécrire $ f(x) $ sous la forme :\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = \\frac{1}{2} \\| r(x) \\|_2^2\n",
        "\\end{equation}\n",
        "\n",
        "Le gradient et la Hessienne de $ f $ se calculent respectivement par :\n",
        "\n",
        "$$ \\nabla f(x) = J(x)^T r(x), \\quad \\nabla^2 f(x) = J(x)^T J(x) + \\sum_{j=1}^{m} r_j(x) \\nabla^2 r_j(x) $$\n",
        "\n",
        "où $ J(x) $ est la matrice jacobienne des résidus, composée des dérivées partielles $ \\frac{\\partial r_j(x)}{\\partial x_i} $.Pour les least squares non linéaire, la méthode de Gauss-Newton exploite la structure de $ f(x) $, négligeant le second terme de la Hessienne $ \\sum_{j=1}^{m} r_j(x) \\nabla^2 r_j(x) $ lorsque les résidus sont petits ou quasi-affines. Ainsi, l'approximation de la Hessienne devient $ J(x)^T J(x) $.\n",
        "\n",
        "Dans cette méthode, le pas de descente est obtenu en résolvant le système linéaire :\n",
        "\n",
        "\\begin{equation}\n",
        "J(x_k)^T J(x_k) p_k = -J(x_k)^T r(x_k)\n",
        "\\end{equation}\n",
        "\n",
        "Le critère de recherche linéaire est basé sur les conditions de Wolfe, avec un ajustement du pas $ \\alpha_k $ via le backtracking.\n",
        "\n",
        "La résolution itérative des moindres carrés utilise efficacement cette simplification de la Hessienne, accélérant la convergence dans les problèmes où les résidus sont proches de zéro.\n"
      ],
      "metadata": {
        "id": "ZpgF5HmCByDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import jacobian\n",
        "from scipy.linalg import cholesky, solve_triangular\n",
        "\n",
        "\n",
        "\n",
        "def gauss_newtone(r, j, x_0):\n",
        "  \"\"\"\n",
        "    Algorithme de Gauss-Newton pour minimiser une fonction de moindres carrés.\n",
        "\n",
        "    :param r: Fonction résiduelle, correspondant à la différence entre les prédictions du modèle et les données observées.\n",
        "    :param j: Jacobienne de la fonction résiduelle, utilisée pour le calcul du gradient.\n",
        "    :param x_0: Point de départ pour la minimisation (vecteur initial des paramètres du modèle).\n",
        "\n",
        "    :return: Solution optimisée après convergence, les paramètres qui minimisent la somme des carrés des résidus.\n",
        "  \"\"\"\n",
        "  grad_f_x0 = j(x_0).T @ r(x_0)\n",
        "\n",
        "  grad_f = lambda x : j(x).T @ r(x)\n",
        "  f = lambda x : np.linalg.norm(r(x)) ** 2\n",
        "  wolf = Wolf(f, grad_f)\n",
        "  i = 0\n",
        "  while np.linalg.norm(grad_f_x0) > 1e-6 and i<100000:\n",
        "    pk = np.linalg.solve(j(x_0).T @ j(x_0),  -j(x_0).T @ r(x_0))\n",
        "\n",
        "\n",
        "    #trouver le alpha respecte les condition de wolf\n",
        "    alpha = wolf.backtracking(x_0, pk)\n",
        "    x_0 += alpha*pk\n",
        "    grad_f_x0 = j(x_0).T @ r(x_0)\n",
        "    i+=1\n",
        "\n",
        "  print(f\"Le nombre d'itération est de: {i}\")\n",
        "  print(f'norme du gradient: {np.linalg.norm(grad_f_x0)}')\n",
        "  return x_0\n"
      ],
      "metadata": {
        "id": "TfElAK_Nsd1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "**Modèle exponentiel :** $$y = a   \\cdot e^{bx} $$"
      ],
      "metadata": {
        "id": "BBvT0_XTFQyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = np.random.normal(3, 0.5, 75)\n",
        "b = np.random.normal(0.1, 0.001, 75)  # Valeurs plus petites pour éviter une explosion\n",
        "\n",
        "# Générer les données\n",
        "x_data = np.linspace(0, 30, 75)  # Réduire l'intervalle de x pour éviter des valeurs trop grandes\n",
        "y_data = a * np.exp(b * x_data)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(x_data, y_data)\n",
        "\n",
        "# Ajout de titres et de labels\n",
        "plt.title('Nuage de points')\n",
        "plt.xlabel('Axe X')\n",
        "plt.ylabel('Axe Y')\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def modele_exponentiel(coef):\n",
        "  return y_data -(coef[0] * np.exp(coef[1] * x_data))\n",
        "\n",
        "j = jacobian(modele_exponentiel)\n",
        "#print(j(np.array([3,0.1])))\n",
        "\n",
        "sol = gauss_newtone(modele_exponentiel, j, np.array([3.5, 0.5]))\n",
        "#print(sol)\n",
        "\n",
        "plt.plot(np.linspace(0, 30, 100),sol[0]*np.exp(sol[1]*np.linspace(0, 30, 100)))\n",
        "# Affichage du nuage de points\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "QftMycK87H5J",
        "outputId": "ba5935a2-16eb-4a05-e4a7-80c2db3718d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le nombre d'itération est de: 24\n",
            "norme du gradient: 6.898153515912435e-08\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABk1klEQVR4nO3deVxU5eI/8M8wwAw7ArK5hWgqrrmg5JIpbpnV1V9mapnZRm5p93azWyndvrnc9ptZmmnltbx2MzOTJE29JgqCqIiSGi4pi4LsMgwzz+8P7kwOw8AZmOHMwOf9etnLOefMOc88TvLxWRVCCAEiIiIiJ+QidwGIiIiIGotBhoiIiJwWgwwRERE5LQYZIiIicloMMkREROS0GGSIiIjIaTHIEBERkdNikCEiIiKnxSBDRERETotBhoic0saNG6FQKHDhwgW5i2LRY489httuu03uYhC1aAwyRE7I8ENcrVbjypUrZudHjhyJXr16yVAyspUffvgBy5Ytk7sYRA6PQYbIiWk0GqxYsULuYpAF69atQ1ZWVqPe+8MPPyA+Pt7GJSJqeRhkiJxYv379sG7dOly9elXuolAd3NzcoFKp5C4GUYvGIEPkxF566SXodLoGW2UuXLgAhUKBjRs3mp1TKBQmXRgXL17Es88+i27dusHDwwOBgYF48MEH6xyLcuLECdx1113w8PBA+/bt8frrr2PDhg11jl3ZtWsXhg8fDi8vL/j4+GDixIk4deqUpM956tQpjBo1yuQ5er2+zmsb+xxDd92BAwfw9NNPIzAwEL6+vnj00Udx48YNs+s//PBD9OzZEyqVCuHh4Zg7dy6KiopMrqk9Rsbw5/Dmm29i7dq1iIyMhEqlwqBBg5CSkmLyvtWrVwOo+fMx/DL46quvMGDAAPj4+MDX1xe9e/fGe++91+BnJGqJXOUuABE1XkREBB599FGsW7cOL774IsLDw5t8z5SUFBw6dAjTpk1D+/btceHCBaxZswYjR45EZmYmPD09AQBXrlzB3XffDYVCgSVLlsDLywuffPJJnS0QX3zxBWbNmoVx48Zh5cqVqKiowJo1azBs2DAcO3as3gGxubm5uPvuu1FdXY0XX3wRXl5eWLt2LTw8PGz6HIN58+bB398fy5YtQ1ZWFtasWYOLFy9i3759xjCxbNkyxMfHIzY2FnFxccbrUlJS8Msvv8DNza3eZ2zevBmlpaV4+umnoVAosGrVKkyePBm//fYb3Nzc8PTTT+Pq1atITEzEF198YfLexMREPPzwwxg9ejRWrlwJADh9+jR++eUXLFy4sMHPR9TiCCJyOhs2bBAAREpKijh//rxwdXUVCxYsMJ6/6667RM+ePY2vs7OzBQCxYcMGs3sBEEuXLjW+rqioMLsmKSlJABCff/658dj8+fOFQqEQx44dMx4rKCgQAQEBAoDIzs4WQghRWloq/P39xZNPPmlyz9zcXOHn52d2vLbnnntOABBHjhwxHsvPzxd+fn42fY6hTgcMGCCqqqqMx1etWiUAiO3btxuf7e7uLsaOHSt0Op3xug8++EAAEJ9++qnx2KxZs0SnTp2Mrw1/DoGBgaKwsNB4fPv27QKA2LFjh/HY3LlzRV1/RS9cuFD4+vqK6urqej8PUWvBriUiJ9e5c2c88sgjWLt2LXJycpp8v1tbOrRaLQoKCtClSxf4+/sjLS3NeC4hIQExMTHo16+f8VhAQABmzJhhcr/ExEQUFRXh4YcfxvXr142/lEolBg8ejJ9//rne8vzwww8YMmQIoqOjjcfatm1r8+cYPPXUUyYtKnFxcXB1dcUPP/wAAPjpp59QVVWF5557Di4uf/wV+uSTT8LX1xc7d+5s8BkPPfQQ2rRpY3w9fPhwAMBvv/3W4Hv9/f1RXl6OxMRESZ+HqKVjkCFqAV5++WVUV1fbZAbTzZs38eqrr6JDhw5QqVQICgpC27ZtUVRUhOLiYuN1Fy9eRJcuXczeX/vY2bNnAQCjRo1C27ZtTX7t3r0b+fn59Zbn4sWL6Nq1q9nxbt262fQ5BrWf5e3tjbCwMOOYn4sXL9b5fHd3d3Tu3Nl4vj4dO3Y0eW0INXWNxant2Wefxe23344JEyagffv2ePzxx5GQkNDg+4haKo6RIWoBOnfujJkzZ2Lt2rV48cUXzc7fOlD0VjqdzuzY/PnzsWHDBjz33HOIiYmBn58fFAoFpk2bZnGAbX0M7/niiy8QGhpqdt7V1TZ/DTXXc2xBqVTWeVwI0eB7g4ODkZ6ejh9//BG7du3Crl27sGHDBjz66KP47LPPbF1UIofnOP9nE1GTvPzyy9i0aZNxAOitDP/irz2rpq7Wg6+//hqzZs3CW2+9ZTxWWVlp9t5OnTrh3LlzZu+vfSwyMhJAzQ/g2NhYSZ+l9nMMrS23qr0+S1OfY3D27FncfffdxtdlZWXIycnBPffcYyyP4fmdO3c2XldVVYXs7OwmPftWlsInUNP6M2nSJEyaNAl6vR7PPvssPv74Y7zyyit1tpIRtWTsWiJqISIjIzFz5kx8/PHHyM3NNTnn6+uLoKAgHDhwwOT4hx9+aHYfpVJp1jLwz3/+06z1Zty4cUhKSkJ6errxWGFhIf71r3+ZXefr64s33ngDWq3W7HnXrl2r93Pdc889OHz4MJKTk03eY+vnGKxdu9bk/WvWrEF1dTUmTJgAAIiNjYW7uzvef/99k3pav349iouLMXHiREnPaYiXlxcA8/BZUFBg8trFxQV9+vQBULNAIlFrwxYZohbkb3/7G7744gtkZWWhZ8+eJueeeOIJrFixAk888QQGDhyIAwcO4NdffzW7x7333osvvvgCfn5+iIqKQlJSEn766ScEBgaaXPfCCy9g06ZNGDNmDObPn2+cft2xY0cUFhYaWxR8fX2xZs0aPPLII+jfvz+mTZuGtm3b4tKlS9i5cyeGDh2KDz74wOJneuGFF/DFF19g/PjxWLhwoXH6dadOnXDixAnjdU19jkFVVRVGjx6NqVOnIisrCx9++CGGDRuG++67D0DNQOMlS5YgPj4e48ePx3333We8btCgQZg5c2aDz5BiwIABAIAFCxZg3LhxUCqVmDZtGp544gkUFhZi1KhRaN++PS5evIh//vOf6NevH3r06GGTZxM5FZlnTRFRI9w6/bq2WbNmCQAm06+FqJlWPWfOHOHn5yd8fHzE1KlTRX5+vtn06xs3bojZs2eLoKAg4e3tLcaNGyfOnDkjOnXqJGbNmmVyz2PHjonhw4cLlUol2rdvL5YvXy7ef/99AUDk5uaaXPvzzz+LcePGCT8/P6FWq0VkZKR47LHHxNGjRxv8vCdOnBB33XWXUKvVol27duLvf/+7WL9+vcn066Y+x1Cn+/fvF0899ZRo06aN8Pb2FjNmzBAFBQVm13/wwQeie/fuws3NTYSEhIi4uDhx48YNk2ssTb/+xz/+YXa/2n8O1dXVYv78+aJt27ZCoVAYp2J//fXXYuzYsSI4OFi4u7uLjh07iqefflrk5OTUX4lELZRCCAmjy4iIJHruuefw8ccfo6yszOKgVke0ceNGzJ49GykpKRg4cKDcxSEiiThGhoga7ebNmyavCwoK8MUXX2DYsGFOFWKIyHlxjAwRNVpMTAxGjhyJHj16IC8vD+vXr0dJSQleeeUVuYtGRK0EgwwRNdo999yDr7/+GmvXroVCoUD//v2xfv16jBgxQu6iEVErwTEyRERE5LQ4RoaIiIicFoMMEREROa0WP0ZGr9fj6tWr8PHxqXfJbyIiInIcQgiUlpYiPDzcZKf52lp8kLl69So6dOggdzGIiIioES5fvoz27dtbPN/ig4yPjw+Amorw9fW12X21Wi12796NsWPHws3NzWb3balYX9KxrqRjXUnHupKOdSWdPeuqpKQEHTp0MP4ct6TFB5lb93uxdZDx9PSEr68vv+gSsL6kY11Jx7qSjnUlHetKuuaoq4aGhXCwLxERETktBhkiIiJyWgwyRERE5LQYZIiIiMhpMcgQERGR02KQISIiIqfFIENEREROi0GGiIiInBaDDBERETktBhkiIiKymk4vkJxdCABIzi6ETi9kKQeDDBEREVklISMHw1buxeOfpQAAHv8sBcNW7kVCRk6zl4VBhoiIiCRLyMhB3KY05BRXmhzPLa5E3Ka0Zg8zDDJEREQkiU4vEL8jE3V1IhmOxe/IbNZuJgYZIiIikiQ5u9CsJeZWAkBOcaVx7ExzYJAhIiIiSfJLLYeYxlxnCwwyREREJEmwj9qm19kCgwwRERFJEh0RgDA/NRQWzisAhPmpER0R0GxlYpAhIiIiSZQuCiydFAUAZmHG8HrppCgoXSxFHdtjkCEiIiLJxvcKw5qZ/RHqZ9p9FOqnxpqZ/TG+V1izlse1WZ9GRERETm98rzCMiQrF4XP5uH76MD6dNQhDugQ3a0uMAVtkiIiIyGpKF4VxLEx0RIAsIQZgkCEiIiInxiBDRERETotBhoiIiJwWgwwRERE5LQYZIiIicloMMkREROS0GGSIiIjIaTHIEBERkdNikCEiIiKnxSBDRERETotBhoiIiJwWgwwRERE5LQYZIiIicloMMkRERNQoer2QuwgMMkRERNQ4Xx79He9nKHH4t0LZysAgQ0RERBbp9AJJ5wuwPf0Kks4XQPe/VphqnR7rD17A+VIFsvJKZSufq2xPJiIiIoeWkJGD+B2ZyCmuNB4L81Nj6aQoaHUCl2/chJerwIMD2slWRgYZIiIiMpOQkYO4TWmoPQomt7gSz2xKQ8cADwDA8FABT3f54gS7loiIiMiETi8QvyPTLMQAMB67VHgTajcXDA/VN2fRzDDIEBERkYnk7EKT7iRLRnQNgrdbMxSoHgwyREREZCK/tOEQAwBDItrYuSQNkz3IXLlyBTNnzkRgYCA8PDzQu3dvHD161HheCIFXX30VYWFh8PDwQGxsLM6ePStjiYmIiFq2YB+1pOu6hfjYuSQNkzXI3LhxA0OHDoWbmxt27dqFzMxMvPXWW2jT5o+Et2rVKrz//vv46KOPcOTIEXh5eWHcuHGorJSWFomIiMg60REBCPNTQ1HPNUHe7hjQSf4WGVlnLa1cuRIdOnTAhg0bjMciIiKMvxdC4N1338XLL7+M+++/HwDw+eefIyQkBN9++y2mTZvW7GUmIiJq6ZQuCiydFIW4TWlQAHUO+n39gV5QutQXdZqHrC0y3333HQYOHIgHH3wQwcHBuOOOO7Bu3Trj+ezsbOTm5iI2NtZ4zM/PD4MHD0ZSUpIcRSYiImoVxvcKw5qZ/RHqZ97NtCi2K8b3CpOhVOZkbZH57bffsGbNGixevBgvvfQSUlJSsGDBAri7u2PWrFnIzc0FAISEhJi8LyQkxHiuNo1GA41GY3xdUlICANBqtdBqtTYru+FetrxnS8b6ko51JR3rSjrWlXSsqz+M7haEkV2HI/XiDWw4dBE/nbmGPu18ETfiNpOfq/aoK6n3VAghZNvxyd3dHQMHDsShQ4eMxxYsWICUlBQkJSXh0KFDGDp0KK5evYqwsD+S39SpU6FQKLBlyxazey5btgzx8fFmxzdv3gxPT0/7fBAiIqIWrKIaWJamhEanwBPddOgdYP/oUFFRgenTp6O4uBi+vr4Wr5O1RSYsLAxRUVEmx3r06IH//Oc/AIDQ0FAAQF5enkmQycvLQ79+/eq855IlS7B48WLj65KSEnTo0AFjx46ttyKspdVqkZiYiDFjxsDNTeZJ9E6A9SUd60o61pV0rCvpWFfmVu/7DRrdOdwe7I2/TI+By//Gxtizrgw9Kg2RNcgMHToUWVlZJsd+/fVXdOrUCUDNwN/Q0FDs2bPHGFxKSkpw5MgRxMXF1XlPlUoFlUpldtzNzc0uX0h73belYn1Jx7qSjnUlHetKOkerK51eIDm7EPmllQj2USM6IqBZBttWVFXjs6SLAIC5o7pApXI3u8YedSX1frIGmUWLFuHOO+/EG2+8galTpyI5ORlr167F2rVrAQAKhQLPPfccXn/9dXTt2hURERF45ZVXEB4ejgceeEDOohMRETWb+jZvtPeg281HLuFGhRadAj0xsbdjDPC9layzlgYNGoRt27bhyy+/RK9evfD3v/8d7777LmbMmGG85oUXXsD8+fPx1FNPYdCgQSgrK0NCQgLUammL9RARETkzw+aNtbcMyC2uRNymNCRk5Njt2ZpqHdb99zcAQNxdkXBVyr6OrhnZd7++9957ce+991o8r1Ao8Nprr+G1115rxlIRERHJr6HNGxUA4ndkYkxUqF26mf6TegV5JRqE+qrxp/7tbH5/W3C8aEVEREQAGt68UQDIKa5EcnahzZ9drdPjo/3nAQBPjegMlavS5s+wBQYZIiIiByV180ap11nj+xM5uFRYgQAvd0yL7mDz+9sKgwwREZGDkrp5o9TrpNLrBVb/fA4AMGdYBDzdZR+JYhGDDBERkYNqaPNGBWpmL0VHBNj0uQmncnE2vww+alc8EtPJpve2NQYZIiIiB2XYvBGAWZgxvF46KcqmA331eoH395wFADw+NAK+asdZS6cuDDJEREQOzNLmjaF+aqyZ2d/m68jszszDmdxS+Khc8fjQCJve2x4ct9OLiIiIANSEmTFRoXZf2VeIP1pjHht6G/w8Hbs1BmCQISIicgpKFwViIgPt+oyfTucjM6cEXu5Kp2iNAdi1RERERDBtjXn0ztvQxst8TyVHxCBDRERE2Jd1DSevFMPDTYknhjlHawzAIENERNTqCSHwrqE1JqYTAr1VMpdIOgYZIiKiVm7/r9dw/HIR1G4ueGJ4Z7mLYxUGGSIiolZMCIF3fqppjZkxuBPa+jhPawzAIENERNSq/ZyVj+OXi+DhpsQzd0XKXRyrMcgQERG1UkIIvJ34KwDg0TudrzUGYJAhIiJqtXZn5iHjSs26MU+PcL7WGIAL4hERETk1nV40asVfvV7gnf+1xsweGoEAJ1k3pjYGGSIiIieVkJGD+B2ZyCmuNB4L81Nj6aSoBvdgSjiVa9xT6YnhzrNuTG3sWiIiInJCCRk5iNuUZhJiACC3uBJxm9KQkJFj8b26W1pjHh8WAX9P52yNARhkiIiInI5OLxC/IxOijnOGY/E7MqHT13UFsPNkDs7ml8FX7Yo5TtwaAzDIEBEROZ3k7EKzlphbCQA5xZVIzi40O1et0+Pdn2paY54a0Rm+asff4bo+DDJEREROJr/Ucohp6Lpvjl3Bb9fK0cbTDY85yQ7X9WGQISIicjLBPupGXaep1uG9/63iO/fuLvBWOf+cHwYZIiIiJxMdEYAwPzUsTbJWoGb2UnREgMnxzUcu4UrRTYT6qjFzSCe7l7M5MMgQERE5GaWLAksnRQGAWZgxvF46KcpkPZlyTTVW/3wOALBgdFeo3ZTNUFL7Y5AhIiJyQuN7hWHNzP4I9TPtPgr1U2PNzP5m68hsPHQB18uqcFugJx4c2L45i2pXzt85RkRE1EqN7xWGMVGhDa7sW1RRhY/2nwcALBpzO9yULacdg0GGiIjIiSldFIiJDKz3mo8P/IbSymp0D/XBpD7hzVSy5tFyIhkRERGZyS+txIZfsgEAfx7bDS4S9mFyJgwyRERELdg/95xDpVaPOzr6Y3SPYLmLY3MMMkRERC1U9vVyfJl8CQDw1/HdoVC0rNYYgEGGiIioxXrzxyxU6wVGdQ/GkM71j6NxVgwyRERELVD65SLsPJkDhQJ4YXw3uYtjNwwyRERELYwQAit2nQYATL6jPbqH+spcIvthkCEiImph9v16DYd/K4S7qwsWj71d7uLYFYMMERFRC6LTC6zcdQYA8Nidt6Gdv4fMJbIvBhkiIqIW5NtjV3AmtxS+alc8OzJS7uLYHYMMERFRC1Gp1eHtxF8BAM/e3QX+nu4yl8j+GGSIiIhaiA2/XMCVopsI81PjsTtvk7s4zYJBhoiIqAUoKNPgw5/PAQD+Mq4b1G5KmUvUPBhkiIiIWoB3fzqLUk01erXzxQP92sldnGbD3a+JiIic3Ln8Mmz+31YEf7snCi4uCuj0AsnZhcgvrUSwjxrREQFQtrANIwGZW2SWLVsGhUJh8qt79+7G85WVlZg7dy4CAwPh7e2NKVOmIC8vT8YSExEROZ4Vu05DpxeI7RGCmMhAJGTkYNjKvXh43WEs/CodD687jGEr9yIhI0fuotqc7F1LPXv2RE5OjvHXwYMHjecWLVqEHTt2YOvWrdi/fz+uXr2KyZMny1haIiIix3Lo/HX8dDofShcFXpzQHQkZOYjblIac4kqT63KLKxG3Kc1imNHpBZLOF2B7+hUknS+ATi+ao/hNJnvXkqurK0JDQ82OFxcXY/369di8eTNGjRoFANiwYQN69OiBw4cPY8iQIc1dVCIiIoei1wv8386arQhmDO6IiCAvPLL+COqKIAKAAkD8jkyMiQo16WZKyMhB/I5Mk/AT5qfG0klRGN8rzL4foolkb5E5e/YswsPD0blzZ8yYMQOXLtX08aWmpkKr1SI2NtZ4bffu3dGxY0ckJSXJVVwiIiKHse3YFZy6WgIflSsWju6K5OxCs5aYWwkAOcWVSM4uNB5rbAuOo5C1RWbw4MHYuHEjunXrhpycHMTHx2P48OHIyMhAbm4u3N3d4e/vb/KekJAQ5ObmWrynRqOBRqMxvi4pKQEAaLVaaLVam5XdcC9b3rMlY31Jx7qSjnUlHetKOmepq3JNNVYm1GxF8PSICPiqXJBfXA6VsuEuofzicmi1vtDpBZbvPAV3C+9RAFi+8xRGdg2sc6CwPetK6j0VQgiH6QQrKipCp06d8Pbbb8PDwwOzZ882CSUAEB0djbvvvhsrV66s8x7Lli1DfHy82fHNmzfD09PTLuUmIiJqbjsvuWD3FRcEqgSW9NPBTfY+FtuqqKjA9OnTUVxcDF9fy7t3yz5G5lb+/v64/fbbce7cOYwZMwZVVVUoKioyaZXJy8urc0yNwZIlS7B48WLj65KSEnTo0AFjx46ttyKspdVqkZiYiDFjxsDNzc1m922pWF/Ssa6kY11Jx7qSzhnq6vcbN/GXlF8A6BH/p34Y1zMEQM2A3XHvHkBeSWWd42QUAEJ81fjxuRFQuijww8kcvPCfEw0+b9WUPrint/lYGXvWlaFHpSEOFWTKyspw/vx5PPLIIxgwYADc3NywZ88eTJkyBQCQlZWFS5cuISYmxuI9VCoVVCqV2XE3Nze7fCHtdd+WivUlHetKOtaVdKwr6Ry5rt5MPImqaj2GdA7AxL7toFDUdPu4AVgysSfiNqUBgEmYMXQMLZnYE2pVzR5MwX5e0OgaXlsm2M+r3rqwR11JvZ+sDVF//vOfsX//fly4cAGHDh3Cn/70JyiVSjz88MPw8/PDnDlzsHjxYvz8889ITU3F7NmzERMTwxlLRETUah35rQA7T+bARQG8em9PY4gxGN8rDGtm9keon9rkeKifGmtm9jeZhRQdEYAwPzUsRRkFamYvRUcE2PhT2I6sLTK///47Hn74YRQUFKBt27YYNmwYDh8+jLZt2wIA3nnnHbi4uGDKlCnQaDQYN24cPvzwQzmLTEREJBudXuC17zMBANOiOyIqvO4hE+N7hWFMVGiDK/sqXRRYOikKcZvSoEDdLThLJ0U59IrAsgaZr776qt7zarUaq1evxurVq5upRERERI7r69TLNdOt1a54fszt9V6rdFEgJjKwwXsaWnBqryMT6iTryDjUGBkiIiKqW2mlFv/48VcAwMLRXRHobT4etLGktuA4IgYZIiIiJ/DeT2dxvUyDzkFeeDTmNpvfX2oLjqNpYbPOiYiIWp6zeaXYeOgCAODVSVFwd+WPbwPWBBERkQMTQmDZjlOo1guMiQrByG7BchfJoTDIEBERObCEjFz8cq4A7q4ueGVilNzFcTgMMkRERA7qZpUOf//fdOtn7opEx0ButVMbgwwREZGD+nDfOVwtrkQ7fw/E3RUpd3EcEoMMERGRA7pYUI6PD/wGAHjl3h7wcFfKXCLHxCBDRETkYIQQeG1HJqqq9RjeNQjjelreLLm1Y5AhIiJyMImZedhzJh9uypotBGrvp0R/YJAhIiJyIBVV1YjfUTPA98nhndEl2EfmEjk2BhkiIiIH8v6ec7hSdBPt/D0wf1RXuYvj8BhkiIiIHMTZvFJ88t+aAb7x9/XkAF8JuNcSERGRRDq9sNvGikIIvPxtBqr1ArE9QhAbFWKT+9Zmz88gBwYZIiIiCRIychC/IxM5xZXGY2F+aiydFIXxvcKafP9tx67gSHYh1G4uWHaffVbwtfdnkAO7loiIiBqQkJGDuE1pJgEAAHKLKxG3KQ0JGTlNun9xhRZv/HAaALBgdFe0b2P7FXzt/RnkwiBDRERUD51eIH5HJkQd5wzH4ndkQqev6wppViScwfWyKnQJ9sYTwzo3+j6WNMdnkAuDDBERtVo6vUDS+QJsT7+CpPMFdf4gT84uNGvFuJUAkFNcieTswkaVIeVCIb5MvgQAeONPveHuavsfzfb+DHLiGBkiImqVLI0XeXViN5Pr8kstB4DGXHcrTbUOS745CQCYNqgDoiMCrL6HFPb8DHJjiwwREbU69Y0XWbQl3eRYsI9a0j2lXnerj/f/hnP5ZQjydseSCT2sfr9U9vwMcmOQISKiVkXKeBHDdQAQHRGAMD81LE1QVqCmJcfa1pRz+aX4596zAIBpgzrCW22/ThJ7fQZHwCBDREStipTxIgCQevEGAEDpUrPfEQCzIGB4vXRSlFVrsew6eRX3vHcQWl3N0z74+RyGrdxrt5lD9vgMjoJBhoiIWhWp40Cul2mMvx/fKwxrZvZHqJ9p10uonxprZva3ag2WhIwcxP3rGKp0epPj9p4GbcvP4Eg42JeIiFoVqeNAgrxVJq/H9wrDmKjQJq2Kq9MLvLr9VJ3nBGpaR+J3ZGJMVKhdWkds8RkcDYMMERG1KobxIrnFlXWOkzH8SB/QqY3ZOaWLAjGRgY1+dk2A0Fg8f+s06KY8pz5N/QyOhl1LRETUqkgZL2K4ztYST+dKus4Zp0HLhUGGiIhanfrGi7zzUD+7PLO4QotvUq9IutYZp0HLhV1LRETUKlkaL6LXVeOHbNs/740fTqPophZKF4XFrQAUqAlTzjgNWi4MMkRE1GrVNV5Er7P9c345dx1bjl6GQgEsiu2Kt3b/CsB03RpnnwYtF3YtERER2VFFVTVe/OYEAODRIZ0wb1TXFjkNWi5skSEiIrKjVQlZuFx4E+38PfCX8d0BtMxp0HJhkCEiohZPpxeyhIbDvxVg46ELAIDlk3vDW/XHj92WNg1aLgwyRETUolna5XrppCibdePUFZQ01Tq88HVNl9LD0R0w4va2NnkWmWKQISKiFsuwy3XtOUKG7QBsMSbFUlDqFuqDS4UVaOfvgZfusd/O1q0dB/sSEVGLJGWX6/gdmRanQkthCEq1N6HMKa7EvqxrAIAVU3rDR+3W6GdQ/RhkiIioRZKyy7VhO4DGqC8oGXi6K3FnZFCj7k/SMMgQEVGLJHWZ/8ZuB9BQUAKAiipdo4MSScMgQ0RELZLUZf4bux2AvYMSScMgQ0RELZJhl2tLk6wVqBmU29jtAOwdlEgaBhkiImqRpOxy3ZTtAOwdlEgaBhkiImqx6tvluqlTr28NSrVx36Tmw3VkiIioRbPndgDje4Vh+eReWLItA+KW6Uuhtyy4J9eqwq2FwwSZFStWYMmSJVi4cCHeffddAEBlZSWef/55fPXVV9BoNBg3bhw+/PBDhISEyFtYIiJyKvbaDkAIgV0ZeRACiAjywvxRXRDm52EMK82xqnBr5xBdSykpKfj444/Rp08fk+OLFi3Cjh07sHXrVuzfvx9Xr17F5MmTZSolERGRqU1HLmH/r9fg7uqCtY8MwOT+7RETGWgMMXUtlmdYVTghI0emUrcssgeZsrIyzJgxA+vWrUObNm2Mx4uLi7F+/Xq8/fbbGDVqFAYMGIANGzbg0KFDOHz4sIwlJiIiAs7ll+L/dmYCAP46vju6hvgYzzXHqsJUQ/aupblz52LixImIjY3F66+/bjyempoKrVaL2NhY47Hu3bujY8eOSEpKwpAhQ+q8n0ajgUajMb4uKSkBAGi1Wmi1WpuV23AvW96zJWN9Sce6ko51JR3rSjopdaWp1mP+5mOo1OoxrEsgZg5qZ3J9cnYhCstuQqW0/JzCsps4fC7fqWc12fN7JfWesgaZr776CmlpaUhJSTE7l5ubC3d3d/j7+5scDwkJQW5ursV7Ll++HPHx8WbHd+/eDU9PzyaXubbExESb37MlY31Jx7qSjnUlHetKuvrqavtFF5zOdYGXq8BYvzwkJOwyu2ZVdMPPuH76MH443ZRSOgZ7fK8qKiokXSdbkLl8+TIWLlyIxMREqNW2WyxoyZIlWLx4sfF1SUkJOnTogLFjx8LX19dmz9FqtUhMTMSYMWPg5sbNwBrC+pKOdSUd60o61pV0DdXVofMF2JuUCgB4c+odiO0RbHZNcnYhHv/M/B/ptX06a5DTt8jY63tl6FFpiGxBJjU1Ffn5+ejfv7/xmE6nw4EDB/DBBx/gxx9/RFVVFYqKikxaZfLy8hAaGmrxviqVCiqVyuy4m5ubXf7ntdd9WyrWl3SsK+lYV9KxrqSrq65ulFfhr9+cAgA8HN0RE/q0q/O9Q7oEI8DbA7nFlXWOk1GgZor2kC7BLWIqtj2+V1LvJ9tg39GjR+PkyZNIT083/ho4cCBmzJhh/L2bmxv27NljfE9WVhYuXbqEmJgYuYpNREStlBACL207idySSnQO8sIr9/aweG1TVhXW6QWSzhdge/oVJJ0v4IDgBsjWIuPj44NevXqZHPPy8kJgYKDx+Jw5c7B48WIEBATA19cX8+fPR0xMjMWBvkRERPayOfkSdmXkwtVFgfem3QFP9/p/hBpWFa69jkxoPevIcN0Z68k+a6k+77zzDlxcXDBlyhSTBfGIiIiaU1ZuKV7bUTPV+oXx3dC7vZ+k91mzqrBh3Zna7S+GdWeauqVCS+VQQWbfvn0mr9VqNVavXo3Vq1fLUyAiImr1blbpMG9zGjTVetx1e1s8MayzVe+XsqpwQ+vOKFCz7syYqNAWMabGlmRfEI+IiMiRvfZ9Js7ml6GtjwpvTe0LFzsEieTsQrMVgG8lAOQUVyI5u9Dmz3Z2DDJEREQW7DyRgy+TL0GhAN6Z2g9B3uazYm0hv9RyiGnMda2JQ3UtEREROYqdJ3Kw5NuaqdbPjIjEsK5BdntWsI+09dSkXteasEWGiIjoFj+dzkO1Hli09QRuavUAgG3HfrfrJo/REQEI81ObTdU2UKBm9pIzL55nL5KDzA8//GDPchARETVZU9dgScjIwaIt6dh+0QXilliRV6Kx647VTVl3prWTHGQmT56Mp556CmVlZfYsDxERUaMkZORg2Mq9eHjdYSz8Kh0PrzuMYSv3Sg4fhplD1QI4kGv647E5dqw2rDsT6mfafRTqp+bU63pIHiNz5MgRPPbYY+jTpw82btyIESNG2LNcREREktliDZbaM4eUCgGd+KMF5NaZQw1Np24sa9adoRqSW2T69u2LlJQUPProoxg7diyef/55FBYWoqSkxOQXERFRc2poDRZAWkvKlSLDbssKRPgIKC1kB3vPHDKsO3N/v3aIiQxkiGmAVYN9XV1dsWzZMnz33Xd477330LZtW7Rp0wZt2rSBv78/2rRpY69yEhER1clWa7AkZOQZ3/FYVx0UFvIDZw45FqunX3/zzTeIi4vDiBEj8Le//Q2urpzBTURE8rHFGizfHruCn07XBBk3F8C/juViDDtWc+aQY5GcQoqKivDss89i+/bteOONN7Bw4UJ7louIiEiSpq7Bcia3BC9+cwIAcE/vUOzNNB8czJlDjktykImKikLHjh2RlpaGbt262bNMREREkhnWYMktrqxznEx9LSkllVo880UqKrV6DO8ahH8+3B+7M66gKjvV5Lr6dqwmeUkOMs8++yyWLFkCpVJpz/IQERFZxbAGS9ymNCgAkzBTX0uKXi/w/L+P40JBBdr5e+D9aXdA6aJAbI8Q/JANfDprEK5XVHPmkIOTHGRefvlle5aDiIio0QxrsMTvyDQZ+FtfS8pHB84jMTMP7koXrJnZH2283E3OR0cEwM3Nze5lp6bhSF0iImoRrFmD5Zdz1/Hmj1kAgPj7e6JPe/9mLi3ZCoMMERG1GIY1WOpzubAC8zanQS+ABwe0x7RBHZqpdGQP3DSSiIhajZtVOjz9RSpuVGjRu50f/v5ALygsLRhDTqHRQaaqqgpZWVmorq62ZXmIiIjsQgiBv/7nBDJzShDo5Y6PHxkAtRsnsDg7q4NMRUUF5syZA09PT/Ts2ROXLl0CAMyfPx8rVqyweQGJiIhsYd1/f8N3x6/C1UWBD2f0R7i/h9xFIhuwOsgsWbIEx48fx759+6BW/7G4UGxsLLZs2WLTwhEREdnCf89ew4pdZwAAr06KwuDO9tn0kZqf1YN9v/32W2zZsgVDhgwx6Vfs2bMnzp8/b9PCERERNdXFgnLM//IY9AKYOrA9HhnSSe4ikQ1Z3SJz7do1BAcHmx0vLy/ngCkiInIopZVazPnsKIoqtOjbwR+v3c/BvS2N1UFm4MCB2Llzp/G14QvxySefICYmxnYlIyIiagKdXmDBl8dwLr8Mob5qrOPg3hbJ6q6lN954AxMmTEBmZiaqq6vx3nvvITMzE4cOHcL+/fvtUUYiIiKrrUo4g5+zrkHl6oK1jw5AsK+0zSXJuVjdIjNs2DCkp6ejuroavXv3xu7duxEcHIykpCQMGDDAHmUkIiKyytepv+PjA78BAN58sC9X7m3BGrWyb2RkJNatW2d2vKKiAp6enk0uFBERUWOlXizES9+cBAAsGNUFk/qGy1wisierW2RGjx6NK1eumB1PTk5Gv379bFEmIiKiRrlcWIGnPk9FlU6P8T1D8Vzs7XIXiezM6iCjVqvRp08f45oxer0ey5Ytw7Bhw3DPPffYvIBERES16fQCSecLsD39CpLOF0CnFyi+qcXsjSkoKK9Cz3BfvDW1L1zq2DCSWharu5Z27tyJ1atX4/HHH8f27dtx4cIFXLx4Ed9//z3Gjh1rjzISEREZJWTkIH5HJnKKK43HQn1V8Pd0N85QWj9rELxU3Be5NWjUn/LcuXPx+++/Y+XKlXB1dcW+fftw55132rpsRETUwuj0AsnZhcgvrUSwjxrREQFQWtFqkpCRg7hNaRC1jueWaJBbooHK1QWfzBqIUD/OUGotrA4yN27cwBNPPIE9e/bg448/xv79+zF27FisWrUKzz77rD3KSERELUBdLSlhfmosnRSF8b3CGny/Ti8QvyPTLMTcytNdiR5hvjYoLTkLq8fI9OrVC3l5eTh27BiefPJJbNq0CevXr8crr7yCiRMn2qOMRETk5AwtKbeGGADILa5E3KY0JGTkNHiP5OxCs/fXdqNCi+TswiaVlZyL1UHmmWeewYEDBxAREWE89tBDD+H48eOoqqqyaeGIiMj51deSYjgWvyMTOn19bS1Afmn9Icba66hlsDrIvPLKK3BxMX9b+/bt8fbbb9ukUERE1HI01JIiAOQUVzbYkhLsI23ci9TrqGWwOsjUVlpairVr1yI6OprryBARkRlbtaRERwQgrJ5BvArUjLmJjgiwpnjk5BodZA4cOIBZs2YhLCwMb775JkaNGoXDhw/bsmxERNQC2KolRemiwIsTutV5zjDvaemkKKtmQZHzs2rWUm5uLjZu3Ij169ejpKQEU6dOhUajwbfffouoqCh7lZGIiJyYoSUlt7iyznEyCgChElpSqnV6fJdeMyhYoQDELTcLtWL2E7UskoPMpEmTcODAAUycOBHvvvsuxo8fD6VSiY8++sie5SMiIiendFFg6aQoxG1KgwIwCTNSW1KEEFjyzUnsOZMPlasLvpgzGDq9aPR6NNRySA4yu3btwoIFCxAXF4euXbvas0xERNTCjO8VhjUz+5uvyCuxJeUfP2Zha+rvcFEAH0zvz3EwZCQ5yBw8eBDr16/HgAED0KNHDzzyyCOYNm2aPctGREQtyPheYRgTFWr1yr6fHszGh/vOAwCWT+6NMVEhzVFcchKSB/sOGTIE69atQ05ODp5++ml89dVXCA8Ph16vR2JiIkpLS+1ZTiIiagGULgrERAbi/n7tEBMZ2GCI+e74Vbz2fSYA4C/juuGhQR2bo5jkRKyeteTl5YXHH38cBw8exMmTJ/H8889jxYoVCA4Oxn333WePMhIRUSt04NdreP7f6QCAx+68Dc+OjJS3QOSQmrSOTLdu3bBq1Sr8/vvv+PLLL61+/5o1a9CnTx/4+vrC19cXMTEx2LVrl/F8ZWUl5s6di8DAQHh7e2PKlCnIy8trSpGJiMgJpF4sxNNfpEKrE7i3TxhevTcKCgUH85K5Ji+IBwBKpRIPPPAAvvvuO6ve1759e6xYsQKpqak4evQoRo0ahfvvvx+nTp0CACxatAg7duzA1q1bsX//fly9ehWTJ0+2RZGJiMhBZV4twewNKbip1eGu29vi7an94MIZSWSB1btf29KkSZNMXv/f//0f1qxZg8OHD6N9+/ZYv349Nm/ejFGjRgEANmzYgB49euDw4cMYMmSIHEUmIiI7yr5ejkc/PYKSymoM7NQGH80cAHdXm/ybm1ooWYPMrXQ6HbZu3Yry8nLExMQgNTUVWq0WsbGxxmu6d++Ojh07IikpyWKQ0Wg00Gg0xtclJSUAAK1WC61Wa7PyGu5ly3u2ZKwv6VhX0rGupHOGusoprsSMdcm4XlaFHqE++HhGP7gq9NBq9c1aDmeoK0dhz7qSek+FEKL+7Ubt7OTJk4iJiUFlZSW8vb2xefNm3HPPPdi8eTNmz55tEkoAIDo6GnfffTdWrlxZ5/2WLVuG+Ph4s+ObN2+Gp6enXT4DERE1TUkV8M9TSuRXKhCsFljQSwcfN7lLRXKqqKjA9OnTUVxcDF9fX4vXyd4i061bN6Snp6O4uBhff/01Zs2ahf379zf6fkuWLMHixYuNr0tKStChQweMHTu23oqwllarRWJiIsaMGQM3N/7f1hDWl3SsK+lYV9I5cl3dqKjCI58eRX5lGcL81PjqiUEI9/eQrTyOXFeOxp51ZehRaYjsQcbd3R1dunQBAAwYMAApKSl477338NBDD6GqqgpFRUXw9/c3Xp+Xl4fQ0FCL91OpVFCpVGbH3dzc7PKFtNd9WyrWl3SsK+lYV9I5Wl0V39RizufHkJVXhmAfFb58cgg6BXnJXSwAjldXjswedSX1fg43gkqv10Oj0WDAgAFwc3PDnj17jOeysrJw6dIlxMTEyFhCIiKyhXJNNWZvSMbJK8UI8HLHv54YjNscJMSQ85C1RWbJkiWYMGECOnbsiNLSUmzevBn79u3Djz/+CD8/P8yZMweLFy9GQEAAfH19MX/+fMTExHDGEhGRk6vU6jDnsxSkXSqCr9oVm+YMRtcQH7mLRU5I1iCTn5+PRx99FDk5OfDz80OfPn3w448/YsyYMQCAd955By4uLpgyZQo0Gg3GjRuHDz/8UM4iExFRE1VqdXjy86M4/FshvFWu+GLOYESF224MI7UusgaZ9evX13terVZj9erVWL16dTOViIiIpNLphdUbQBpCzH/PXoenuxIbZg9C3w7+zVNgapFkH+xLRETOJyEjB/E7MpFTXGk8FuanxtJJURjfK6zO91RqdXj6i1RjiNk4OxqDbgtoriJTC+Vwg32JiMixJWTkIG5TmkmIAYDc4krEbUpDQkaO2Xs01To8sykV+3+9Bg83JT59bBCiIxhiqOkYZIiISDKdXiB+RybqWknVcCx+RyZ0+j+uqNTqELcpDfuyrkHt5oJPHxuEIZ0Dm6W81PIxyBARkWTJ2YVmLTG3EqjZaiA5uxBATYh56otU7D2TbwwxMZEMMWQ7HCNDRESS5ZdaDjG1r6uoqsYTnx3FofMF8HBTYv1jA3FnZJCdS0itDYMMERFJFuyjlnSdj8oNj21IQXJ2IbzcldgwO5pjYsgu2LVERESSRUcEIMxPDUuTrBUAQnxV+ODnc0jOLoSPyhWfzxnMEEN2wyBDRESSKV0UWDopCgDMwowCNWNk1K5KpF26UbNi7xODMaBTmwbvq9MLJJ0vwPb0K0g6X2AyWJioPuxaIiIiq4zvFYY1M/ubrSPT1kcFVxcFLhZWIMDLHZ8/Ho1e7fwavF9j1qQhMmCQISIiq43vFYYxUaHGlX0VUOCtxCxcLKhAiK9K8t5JhjVpare/GNakWTOzP8MM1YtdS0RErYytunGULgrERAaidzs/LN91GhcLKtC+jQe2Pn2npBDTmDVpiGpjiwwRUSvy0+k8vLYzy2bdOKeuFmPWp8m4XlaFyLZe+NcTQxDqJ21mkzVr0nDtGbKELTJERK3Ioi3pVm0tUJ/DvxVg2seHcb2sClFhvvj30zGSQwxg3Zo0RJawRYaIqBUwdM9Y6sZRAFj23Sn4qN1wvUzT4G7WiZl5mLs5DVXVekRHBOCTWQPhq3azqkxS16SReh21TgwyREStQOrFG/WeFwBySzSY8ckR4zFLXU5bj17Gi9+chE4vENsjBB9MvwNqN6XVZTKsSZNbXFlnwFIACPVTcw0aqhe7loiIWoHrZRqr31O7y0kIgY/3n8dfvj4BnV5gSv/2+Ghm/0aFGKDhNWkAYOmkKIutQkQAgwwRUasQ5K2y+j23zhzSVuvx2veZWL7rDADgyeER+Mf/6wNXZdN+jBjWpKk9tibUT401M/tjTFQoF8qjerFriYioFRjQqQ1+PG3e8tEQw8yhmeuP4Mj/drR+6Z7ueHJ4ZygUtmkpqb0mjWF8TmJmLoat3MuF8qheDDJERK3Ard0zhq0ErHEkuxBuSgXefLAv7u/XzqZlA/5Yk8aAC+WRVOxaIiJyctYscPfOQ/2smiJt4OGmxGezo+0SYmrjQnlkDbbIEBE5MWv3KYrtEYKxvdoZu3GCvFR4futx5JXUPXMIAFwUwNZnYiTtm2QLXCiPrMEWGSIiJ2XofrF2gTtDN879/dphaNcgLLuv7plDBvH39URpZbXkAbdN3QKBC+WRNdgiQ0TkJHR6YdKSsuw7y90vCtR0v4yJCm1w+rKl3azdXV0wPboDPtx3XnKLjy12suZCeWQNBhkiIidQV0Coj7XdL2OiQpFy4QbWH8wGAIy8vS2m9G+PBV8dkzzg1lYDdLlQHlmDXUtERA7OUheSFFK6X8o01Xjq86PGEPPnsbfjk1kD8cau05IH3NpygC4XyiNrMMgQETmw+gKCFA11v1wurMCUDw9hz5l8qFxd8M+H78C8UV2RcuGG5AG3gHUDdKVoaKE8Tr0mA3YtERE5sIYCgiVSul9SLxbiqc9TUVBehbY+Kqx7dCD6dfAHYP2AW3sM0LW0UB5bYuhWDDJERA6sMTNzpHS//Cf1dyz55iSqdHpEhfli/WMDEebnYTxv7YBbew3Qrb1QHlFtDDJERA6sMTNzQuuZJVSt02PFj5nG8TBjo0Lw7rR+8HQ3/XFg7YBbDtAluTDIEBE5MCkBIcRXhbem9sP1Mk293S/lWuCJL47hl/MFAIAFo7viudFd4VLHtYYBt3Gb0sy2NKirxcfa64lshYN9iYgcmJQZPMvu64mhXYJwf792iIkMrDMsnM0vw9snlfjlfAE83JT4cEZ/LB5ze50hxsDaAbccoEtyYIsMEZGDs7RgXX1dSLfadTIHf956HOVVCrT3V2Pto4MQFe4r+dnWDLjlAF1qbgwyREROoDEBoVqnx5u7f8VH+88DALr46vGvZ4YgxN/LqmdbO+CWA3SpOTHIEBE5CWsCQmF5FRZ8eQwHz10HAMwZ2gk9decR4OVuzyISNTuOkSEiamFO/F6ESf88iIPnrsPDTYn3H74DL47vBiV7d6gFYosMEVELIYTAv45cwms7MlGl0+O2QE989MgAdA/1hVarlbt4RHbBIENE1AKUa6rxt20n8W36VQBAbI8QvDW1L/w83GQuGZF9McgQETm5c/mliNuUhrP5ZVC6KPDCuG54akRnKBTsS6KWj0GGiMiJfZP2O17+NgMVVToE+6jwwfT+XD2XWhUGGSIiJ1RRVY1Xt5/C16m/AwBiOgfi/YfvQFsflcwlI2peDDJERE4mK7cUczen4Vx+GVwUwMLRt2PeqC5cdI5aJQYZIiInIYTAl8mXEb/jFDTVegT7qPDetDu4+By1arKuI7N8+XIMGjQIPj4+CA4OxgMPPICsrCyTayorKzF37lwEBgbC29sbU6ZMQV5enkwlJiKqm04vkHS+ANvTryDpfAF0+rq2eGy8oooqxG1Kw0vbTkJTrcddt7fFroXDGWKo1ZO1RWb//v2YO3cuBg0ahOrqarz00ksYO3YsMjMz4eVVs4T2okWLsHPnTmzduhV+fn6YN28eJk+ejF9++UXOohMRGSVk5JjtgxQmcR8kKZLOF2DRlnTkllTCTanAC+O6Y86wiHo3fCRqLWQNMgkJCSavN27ciODgYKSmpmLEiBEoLi7G+vXrsXnzZowaNQoAsGHDBvTo0QOHDx/GkCFD5Cg2EZFRQkYO4jaloXb7S25xJeI2pTVp12etTo/3fjqL1fvOQQggIsgL70+7A73b+0l6v04vjHszBXlyJAG1TA71zS4uLgYABATUTB1MTU2FVqtFbGys8Zru3bujY8eOSEpKqjPIaDQaaDQa4+uSkhIAgFartenKloZ7cbVMaVhf0rGupJO7rnR6geU7T8FdWXc3kgLA8p2nMLJroNUDcX+7Vo4//+ckTl6p+TvswQHt8LcJ3eClcpX0eX86nYcVu84gt6SmlUjlIvD3gUDiqasY0zPcqrK0NnJ/r5yJPetK6j0VQgjbduQ2kl6vx3333YeioiIcPHgQALB582bMnj3bJJgAQHR0NO6++26sXLnS7D7Lli1DfHy82fHNmzfD09PTPoUnIrIRIYCDeQpsv+gCrV4BT6XA1Eg97gh0iL+qiZpNRUUFpk+fjuLiYvj6+lq8zmFaZObOnYuMjAxjiGmsJUuWYPHixcbXJSUl6NChA8aOHVtvRVhLq9UiMTERY8aMgZsblwBvCOtLOtaVdHLX1Q8nc/DCf040eN2qKX1wT++Gu5fySzV4adsp7M+u2bF6aGQgVkzuiVBfteQy6fQC4949YGyJMahpkdHj1aMu8Pf2wI/PjeB0bQvk/l45E3vWlaFHpSEOEWTmzZuH77//HgcOHED79u2Nx0NDQ1FVVYWioiL4+/sbj+fl5SE0NLTOe6lUKqhU5gtCubm52eULaa/7tlSsL+lYV9LJVVfBfl7Q6BoOA8F+Xg2Wb8fxq3hlewaKKrRQubrgxQndMSvmNqsH9B49X4CLNzSo6dgyV6lX4OINDY79XsoZTw3g/4PS2aOupN5P1unXQgjMmzcP27Ztw969exEREWFyfsCAAXBzc8OePXuMx7KysnDp0iXExMQ0d3GJiExERwQgzE9tITLURIkwP3W9WwYUlldh7r/SMP/LYyiq0KJnuC++nz8Ms4c2blZSfmllwxdZcR2Ro5O1RWbu3LnYvHkztm/fDh8fH+Tm5gIA/Pz84OHhAT8/P8yZMweLFy9GQEAAfH19MX/+fMTExHDGEhHJTumiwNJJUYjblAYFYDJzyRBBlk6KstiFs/tULl7adhLXy6rg6qLAvFFdMPfuLnBTNv7fmME+0rqhpF5H5OhkDTJr1qwBAIwcOdLk+IYNG/DYY48BAN555x24uLhgypQp0Gg0GDduHD788MNmLikRUd3G9wrDmpn9zdaRCa1nHZnC8irE7ziF7elXAQC3h3jjrQf7SZ5WXR9DK1FucaXZlHBAWisRkTORNchImTClVquxevVqrF69uhlKRERkvfG9wjAmKtS4ZkuwT01QMLTE3LqeS/a1cnxx+CIKyqvgogCeHNEZi2Jvh9pNaZOy1NdKZFBfKxGRs3GIwb5ERM5O6aKoc/BsXav+AkC4nxofzhyAfh38bV4WS61EAPDOQ/1sstowkaNgkCEispOEjBw8symtznNXiyuRW3wTsEOQAcxbiYI8XXH99GHE9gixy/OI5MIgQ0RkBzq9wCvfZlg8rwCw7LtT8FG74XqZxqw7yhZubSXSarX44bTNbk3kMBhkiIhsTKvT49Xtp3CtrMriNQJAbokGMz45Yjxmy40miVoLWdeRISJqaVIv3sCkfx7El8mXrH6vYaPJhIwcO5SMqGViiwwRkQ0UV2ixIuGMMcB4q1xRpqm26h4CzdPlRNSSMMgQETWBEALfpl/B69+fRkF5TVfSgwPa44Xx3XHfBwctrudi8X5glxORNdi1RETUSGdyS/DQx4exaMtxFJRXoUuwN7Y8NQT/eLAv2vqosHRSFABLux5Jxy4nIsvYIkNErdqti9VJ7cYpqdTi3cSz+CzpAnR6AQ83JeaN6oInh3eGu+sf/z6sbz0Xaxi6nOJ3ZGJMVCi7mYhuwSBDRK1WXYvV1deNo9cLbDt2BSsSzuBaqQYAMKFXKF6+Nwrt/D3qfIbZei5eKjy/9TjySqzvcsoprkRydiF3rSa6BYMMEbVKCRk5iNuUZhYmDN04a2b2Nwkz6ZeLsOy7U0i/XAQACPZRYc6wCDwxvHODLSS1V/1ddl/9WwjUh7tWE5niGBkianV0eoH4HZl1hgjDsfgdmdDpBfJLK/HnrcfxwOpfkH65yDjeJb9Ug+W7zmDYyr1Wj10xdDmF+lm/AzV3rSYyxRYZImp1krML6x2zYujGeWnbSXx//CrKq3Qm525lqQWnIdZ2OSlQs6M2d60mMsUWGSJqNJ1eIOl8AbanX0HS+QLo9NZ2lMhDavfMlpTLKK/SoU97PwR6udd5Te0WHGsYupzu79cOQ7sGYdl9dc9yMrzmrtVE5tgiQ0SNYu1AWUcitXsmwMsdr94bhbY+KpN1XWqz1UBcS7OcQp2kXonkwCBDRFazdqCso4mOCECYn7rexeq8Va448Je74a12xfb0K5Lua4uBuLW7nLiyL1H92LVERFaxZqCso1K6KLB0UlS9M4befLAPvNU1/9aT2oJjq4G4t3Y5xUQGMsQQ1YNBhoisInWgbHJ2YfMVykplmmpk5ZZB7Wb+V2CYnxof1WpRMrTgWIoTiv+9jwNxiZofu5aIyCpSu0/kWu+kvpV6NdU6bD5yCR/sPWfcF6lXO1/c37cdgn1VFrtxDC04da39woG4RPJikCEiqzR3N4s1LA1AfnliD9zU6vFO4q+4UnQTABAR5IXnx96Oe3qFwUVCAOFAXCLHxCBDRFZpaKCsXOudWBqAnFNcibmbjxlfh/iq8Fzs7fh/A9rDTWld7zoH4hI5HgYZIrKKvbpZGrN5463vtTQA2Vg2BfDCuO547M7b4OGutKpst6q93QARyYtBhoisJqWbxZpg0pg1aQz3B4DNRy42uLu0EEC/Dv5NCjFE5HgYZIioUerrZrEmmDRmTRrD/QvLbmJVNLAiIQvm6+Ga44aLRC0Pp18TUaPVtd6JIZjUbiExBJNbN1hszJo0t95fCCC9QAGtXlp5ueEiUcvDIENENmNtMLF2TZra99fqgQ2/KiEaaI3hOi9ELRe7lojIZqwJJjGRgZK7en45dw35pZXILbppcn8BBTyUAlV6QCfqDjOGo69M7MHZRkQtEIMMEdmMtYvlSe3q+eDn83UeVyoElvbXYWmaEjpd3e8N9VPjvr5h+PvO0065wSUR1Y9BhohsxtrF8qRs3lgfVxfAo46/xV6Z2ANBPjUr9d4or8Lczc67wSUR1Y9jZIjIZqzdk8iwJo3hXFMZ7v/Y0Ajc368doiMC8Pedzr3BJRHVj0GGiGymvmBiabE8w5o0Ib6qJj27rvu3hA0uiah+DDJEZFOGYBLqZ9rNFOqnrrMbR6vTQ1OtR5BP04JMXfd39A0uiajpOEaGiGxOyp5EN8qr8GXKJXx+6CJyS2qChMrVBZP7t8eATm3w563HG3zOX8d1A25k4tNZgzCkS7DZLKSmbHDZlC0TiKj5MMgQkV1Y2pPodE4JNv5yAd+mX4GmumYlu7Y+Kjw6pBNmDOmEAC936PQCb+3OanBjyumDO+HHhEyLIaOxG1w2ZssEIpIHgwwR2Z1Wp8dPmXnYeOgCjtwyHqVnuC9mD43ApL5hULn+sQeSrTambMx9GrNlAhHJh0GGiOwmv6QSXyZfxubki8gr0QCoCRfje4Vi9p23YUCnNlAo6g4jUjam1Gq1DZZByn0MGlqZWIGaWU5jokLZzUTkIBhkiMimhBBIOl+Afx25hB9P5aL6f1Obg7zdMW1QR0yL7oDLhTdxpegmtDpR79gTKWNtpJB6H2tXJiYi+THIEJFNFJZX4T+pv+PL5Ev47Xq58fjATm3wSEwnjO8Vip/P5OPBj5KsGntiaayNtaTch7OciJwPgwyRg0rOLsT1imq7zJix1YwcIQSSfivAlpTL2JWRi6r/Dd71clfigTvaYcbgTogK9wXgHGNPmjLLiYjkwSBD5GB+Op0HAHj8sxRodDXhwpYzZmwxIye/tBJfp/6Of6dcxoWCCuPxnuG+mDG4E+7rFw5v1R9/vTjL2JPGznIiIvlwQTwiB5KQkYNFW9LNjhtaLRIycpp8/7hNaWbjQKTcX6vT48dTuXjisxTELN+LVQlZuFBQAW+VK6YP7ojtc4fi+/nDMH1wR5MQAzjPCruNWZmYiOTFFhkiB2HvVovG3v9Mbgm2Hv0d3x67goLyKuPx/h39MS26Iyb2DoOXqv6/Spxp7Ik1s5yISH6yBpkDBw7gH//4B1JTU5GTk4Nt27bhgQceMJ4XQmDp0qVYt24dioqKMHToUKxZswZdu3aVr9BEdmJotVAp6z7f1Bkz1rSKdAn2xvb0K/gm7Qoyc0qM1/iqXXHX7W0xb1RXdAv1kfxsZxt7YqvZUkRkf7IGmfLycvTt2xePP/44Jk+ebHZ+1apVeP/99/HZZ58hIiICr7zyCsaNG4fMzEyo1Y7xFx6Rrdi71ULq+/6+MxNZuaXGHaGVLgq4uiigqdajpLIaO07k4OjFG1a1Tjjj2BNbzZYiIvuSdYzMhAkT8Prrr+NPf/qT2TkhBN599128/PLLuP/++9GnTx98/vnnuHr1Kr799tvmLyyRndm71ULq+zKvlkCnF+jbwR/TBnWATi+MWwkYWDtmh2NPiMheHHaMTHZ2NnJzcxEbG2s85ufnh8GDByMpKQnTpk2r830ajQYajcb4uqSkpllcq9VKWgVUKsO9bHnPloz11bA72vugUxsVispuAgBULqZtFwoAIb5q3NHep1H1aLh/bnEl9AD0AtAJw51rKF2AJ4ZG4E93hOO2QE+Me/cAVMq62lBq3rV85ymM7BooKYCM7haED6f3xYpdZ4ybRAJAqK8aL07ojtHdgqz+XPxeSce6ko51JZ0960rqPRVCiLr/lmpmCoXCZIzMoUOHMHToUFy9ehVhYX80X0+dOhUKhQJbtmyp8z7Lli1DfHy82fHNmzfD09PTLmUncnRCAFcrgLQCFxy7rkCB5o/g4akUuCNIYGCQHhE+gIUdA4iImlVFRQWmT5+O4uJi+Pr6WrzOYVtkGmvJkiVYvHix8XVJSQk6dOiAsWPH1lsR1tJqtUhMTMSYMWPg5uZms/u2VKwv6RJPXYX2YjpeOeoCjb4mVRhaLWJ7hEi+jxACp3NLseGXizj0WwHyS/+YcaQAoFAIuCiAagH8XuWBmX1N7//DyRy88J8TDT5n1ZQ+uKe3PDN5+L2SjnUlHetKOnvWlaFHpSEOG2RCQ0MBAHl5eSYtMnl5eejXr5/F96lUKqhUKrPjbm5udvlC2uu+LRXrq2Fjeobjh4vpWPNItNUr+wohcDqnFLsycrD16GXklmhMzqtcXfBITCc8N/p2nLxSXO+MnGA/L+OCfPUJ9vOS/c+U3yvpWFfSsa6ks0ddSb2fwwaZiIgIhIaGYs+ePcbgUlJSgiNHjiAuLk7ewhE1g+iIAEn/IwshcPz3Yvx4Khe7TuaYrLRbW1W1Huv/m42Bndo0OOPIGWcaEVHrI2uQKSsrw7lz54yvs7OzkZ6ejoCAAHTs2BHPPfccXn/9dXTt2tU4/To8PNxkrRmi1kir0yM5uxA/nsrF7lN5JoNnVa41kxFrzzQCrFtYzzDTKG5TGhT/e68BZxoRkaOQNcgcPXoUd999t/G1YWzLrFmzsHHjRrzwwgsoLy/HU089haKiIgwbNgwJCQlcQ4ZapTJNNQ78eg2JmXnYeyYfxTf/GNHv5a7EyO7BmNArFF7urpi9McXifaxZWI+r3BKRo5M1yIwcORL1TZpSKBR47bXX8NprrzVjqYgcx5Wim9h7Jh8/ZeYh6XwBqnR/tLL4qF0xoVcoxvcKxZ2RQVC71SwJvD39iqR7S10gj6vcEpEjc9gxMkSOSqcXdvuhrtMLHLtchJ2XXLDmg0M4k1dmcl7pojCuuFtaWY3/nr2OUd2DjSEGsM/CelzllogcFYMMkRUSMnLMulnCmtjNcqO8CgfOXsPPZ/Kx/9druFGhRc2i22VwUQD9O7ZB+zYe+Db9qjHEGBhW2F0zs7/x+U0ZpGvPkEZEZA8MMkQSJWTkIG5Tmlk4qCtM1EenFzjxexH2/3oNB369hvTLRbg1n/ioXRHpVYUZI/tgdFQY/DzcMGzl3jrvVdfg3cYO0rVHSCMisjcGGSIJdHqB+B2ZdbZwSJkJdLXoJg6evY4DZ6/h4LnrKKowXXq7e6gPRnYLxt3d2qJ3uDcSf0zAPf3C4ebmhqTzBZJ3rTZ0/1g7SNdWIY2IqLkxyBBJkJxdaFWYKNNUIzm7AP89ex3/PXsd5/JNx7r4qF0xrEsQ7rq9LUbc3hbh/h7Gc7X3F2nsrthSB+k2NaQREcmJQYacVnOO55AaJjYeysZbu7OQfrkI1XrLM/K83JW4v1+4MWikXCg0fobamjJ4V8ogXWtDGhGRI2GQIafU3OM5pIaJH0/lGX/fKdATHdt44r/nrptdl1eiwTOb0uDv6WbSzRTmp8arE7uZXGvvFXYb2+JDROQIXOQuAJG1DOM5arciGMZzJGTk1Pk+nV4g6XwBtqdfQdL5AuMMIEvHb9W/oz8CvdzrLZeLAri3TxhWTumN/75wN/Y+PxLnrpXVea3hCbXHyuQWV2LRlnSTY4bBu8Afg3UNbLHCrj2maxMRNRe2yJBTaex4DkstOPf1DcN3x3PMjr90T3eE+HogObsAyRduIPVCIcqrdPWWbfX0OzC2ZxiSswuRdukGdp/KrbfLpi63fi6dXsCw05I9V9jlnkpE5MwYZMhhSBnz0pjxHJZm5OQUV+LjA9lm98gprsT8L9PNjvuqXXFbkBeyr5ejtLLaeNzQpQUAw1butTq81PUZACD14g0MvT3EeNxeK+xyTyUicmYMMuQQpI55sXY8R30tOA1xUQCDbgtA+zYeGHhbAKb0bw93V5c6A1diZm6dYakprpdpzI7Za4Vd7qlERM6KQYZkZ80aJtaO52ioBac+egEcyS7EkWzgP2lX8P6es8Yf6reGiaaEpfoEeatsfMf6cU8lInJGDDJkN7e2XAR51v1VkzLmZdl3p+CjdsP1Mg2CvFQI9VUjr6Tu8RwAEOTtjkuF5fj+xFXsy7pmo09jeXG4poSluhhiw4BObSS/x1ZT0bmnEhE5GwYZahJLP0BrdxWplAKrooGfTudhQp/2xvdLGfOSW6LBjE+OGI/5e7rV2/pxvawKf/3PyaZ+tDrLUtdg4sZMSzZMu7Y0JgWA5CDCrQWIqDVjkKFGq28m0NoD2XWGjUVb0qFwURp/wDYmBBimLNcOAQYebkr0bueHfh390budH/7+fSaulWps0vVT12Biqd1dr0zsgSAflcm4mrrGpLw6sRuqslMl3ZNbCxBRa8cgQ41i7UygW93aotGUtUkEagbkhvl5oFuoD0b3CMYdHdogsq0X0i4V1XRpeauwbFJPzN1sPiOnKW4NYFKnLz82NMKklcXSmBS9rho/1F+FALi1ABERwCBDEpmMd/FSYdl3jRvcWrtF446O/gjydsf1sqpGlWvDY9G4q1tb4+uEjBzM+SzFrJXoqRERda4XU9c6MlLcGsCaMn25rjEp+vqXqzHi1gJERAwyJEFdXUiNUaatmQkEAO/89Cte31mNs3llqNLpG33Popt/BKD6ulnWHsjG6ul3oI2Xymw8zwvje5iEtOe3Hrc4mNjS4nByTF/m1gJERAwyVEvtwbs3yqswd3PT10fR6IC/Hf3j65acXWj8vZe7EmF+alwtrkRFA6vn1mZoGZHSzfL3nadx8K+jzFpGareKLLuvca0rzT19mVsLEBExyDQbe+/UbO39LS3qVrtFwUVhq3Eliv/9V8DdVYmnR3RGVLgfosJ80b6NB1xcFGbdV9a0jNiym6UprSvNOX2ZWwsQETHINIv6psfa4l/w1t6/rsBSexdmgzr2T2wUNxeB/xuow6upSrw9ta+xi+dK0U20a+MBoGktI7buZnGGxeG4tQAREYOM3dU3buOZTWlmAaKh9T+kdv1Yur+lwFLXsaZ4avht2HHij00TXRSASgnMvrMT/r7ztKQ1T6xpGbFHN4szLA7HrQWIqLVjkLGjhsZtAOYBor71P+pqebHU9WPp/rYOLLXdGkr+OiHKZGXf66cPY8Ohi6jUmbYQ1PeZpbaMtOZuFmdoPSIishcGmUYwtIoANWMzhnQJrvOHRmOWrre0/oellh1bdf00hgJAiK8Kb03th+tlGrMfoLe2aFRqqvDjacuhq741T6S0jLT2bhZnaD0iIrIHF7kL4GwSMnIwbOVePP5ZCgDg8c9SMGzlXiRk5Jhd29hpr7cOTAXstylhUxjiwLL7emJolyDc368dYiIDLQaF1Is36r1f7c/cGIZullA/0+6jUD81V7glImqh2CJjhVtbRVTKP45b6hpp6rTXlAsFuF6mwS/nrtt0U8LGcFGYtv5YOwbjeplG0nVNXfOE3SxERK0Lg4xEUpeDH9U9BKkXbxinEDe0U3N93k4827RC24Dhx/8HD9e9mJxUQd4qXJdwnS3WPGE3CxFR68EgI5HUdUoGv/ETbtwyoNbDzaXRXUIBXu7w93CDl8oVJ68UN/Iu0hjGldSe1WSr2S8DOrXBj6dNd3eu/fyWOhiXiIjsh0FGotpdHnoBHMxVoLrW6vo3as0Kuqlt3PL7fh6uKCyvQmF5zRL8tbt2aqt93hBI6hr4Wl9gsVe3zK33aI2DcYmIyD4YZCSq3eWhE8DWbKWFq815q5R47M4IuCoV6NLWG2N7hmLvmTyLC9MV36w2eb+lEFNf109dC99JCSz27JZ556F+eG1nFtc8ISIim2CQkaj2OiUuAHq20eNUkQJ60XArQplGh6FdgkxCQu2BqYZl+QHLa71YM+i2oYGvcowjie0RgrG92nEwLhER2QSDjES11ylRugBPdNfjhWQlNBL3OaxrRs6tA1OTzhcgt6T+WTt6AbwysQeCfFSSQoAjDnx1xDIREZFz4joyVrC0TkmAl5uk9zc0I0fq1OMgH1WD67YQERG1BmyRsZKhu+bwuXxcP30Yn84ahEGd2+Kuf/zc5OXx7bFfEBERUUvGFplGULoojKEkOiIA7q4uWDopCoD59GJrZuQYxuHUN0U5jFOUiYiIjBhkbMQWy+MbxuEATQtERERErQW7lmzIFsvjGwKRpWnTnKJMRET0BwYZG7PFjBzuF0RERCQNg4yD4hRlIiKihnGMDBERETktBhkiIiJyWk4RZFavXo3bbrsNarUagwcPRnJystxFIiIiIgfg8EFmy5YtWLx4MZYuXYq0tDT07dsX48aNQ35+vtxFIyIiIpk5fJB5++238eSTT2L27NmIiorCRx99BE9PT3z66adyF42IiIhk5tBBpqqqCqmpqYiNjTUec3FxQWxsLJKSkmQsGRERETkCh55+ff36deh0OoSEhJgcDwkJwZkzZ+p8j0ajgUajMb4uKSkBAGi1Wmi1WpuVzXAvW96zJWN9Sce6ko51JR3rSjrWlXT2rCup93ToINMYy5cvR3x8vNnx3bt3w9PT0+bPS0xMtPk9WzLWl3SsK+lYV9KxrqRjXUlnj7qqqKiQdJ1DB5mgoCAolUrk5eWZHM/Ly0NoaGid71myZAkWL15sfF1SUoIOHTpg7Nix8PX1tVnZtFotEhMTMWbMGLi5udnsvi0V60s61pV0rCvpWFfSsa6ks2ddGXpUGuLQQcbd3R0DBgzAnj178MADDwAA9Ho99uzZg3nz5tX5HpVKBZVKZXwthAAA3Lx506aVrNVqUVFRgZs3b6K6utpm922pWF/Ssa6kY11Jx7qSjnUlnT3r6ubNmwD++DluiUMHGQBYvHgxZs2ahYEDByI6OhrvvvsuysvLMXv2bEnvLy0tBQB06NDBnsUkIiIiOygtLYWfn5/F8w4fZB566CFcu3YNr776KnJzc9GvXz8kJCSYDQC2JDw8HJcvX4aPjw8UCtttumjosrp8+bJNu6xaKtaXdKwr6VhX0rGupGNdSWfPuhJCoLS0FOHh4fVepxANtdlQnUpKSuDn54fi4mJ+0SVgfUnHupKOdSUd60o61pV0jlBXDr2ODBEREVF9GGSIiIjIaTHINJJKpcLSpUtNZkiRZawv6VhX0rGupGNdSce6ks4R6opjZIiIiMhpsUWGiIiInBaDDBERETktBhkiIiJyWgwyRERE5LQYZBpp9erVuO2226BWqzF48GAkJyfLXSSHs2zZMigUCpNf3bt3l7tYDuPAgQOYNGkSwsPDoVAo8O2335qcF0Lg1VdfRVhYGDw8PBAbG4uzZ8/KU1iZNVRXjz32mNl3bfz48fIUVkbLly/HoEGD4OPjg+DgYDzwwAPIysoyuaayshJz585FYGAgvL29MWXKFLONeVsDKXU1cuRIs+/VM888I1OJ5bVmzRr06dMHvr6+8PX1RUxMDHbt2mU8L+f3ikGmEbZs2YLFixdj6dKlSEtLQ9++fTFu3Djk5+fLXTSH07NnT+Tk5Bh/HTx4UO4iOYzy8nL07dsXq1evrvP8qlWr8P777+Ojjz7CkSNH4OXlhXHjxqGysrKZSyq/huoKAMaPH2/yXfvyyy+bsYSOYf/+/Zg7dy4OHz6MxMREaLVajB07FuXl5cZrFi1ahB07dmDr1q3Yv38/rl69ismTJ8tYanlIqSsAePLJJ02+V6tWrZKpxPJq3749VqxYgdTUVBw9ehSjRo3C/fffj1OnTgGQ+XslyGrR0dFi7ty5xtc6nU6Eh4eL5cuXy1gqx7N06VLRt29fuYvhFACIbdu2GV/r9XoRGhoq/vGPfxiPFRUVCZVKJb788ksZSug4ateVEELMmjVL3H///bKUx5Hl5+cLAGL//v1CiJrvkJubm9i6davxmtOnTwsAIikpSa5iOoTadSWEEHfddZdYuHChfIVycG3atBGffPKJ7N8rtshYqaqqCqmpqYiNjTUec3FxQWxsLJKSkmQsmWM6e/YswsPD0blzZ8yYMQOXLl2Su0hOITs7G7m5uSbfMz8/PwwePJjfMwv27duH4OBgdOvWDXFxcSgoKJC7SLIrLi4GAAQEBAAAUlNTodVqTb5X3bt3R8eOHVv996p2XRn861//QlBQEHr16oUlS5agoqJCjuI5FJ1Oh6+++grl5eWIiYmR/Xvl8LtfO5rr169Dp9OZ7b4dEhKCM2fOyFQqxzR48GBs3LgR3bp1Q05ODuLj4zF8+HBkZGTAx8dH7uI5tNzcXACo83tmOEd/GD9+PCZPnoyIiAicP38eL730EiZMmICkpCQolUq5iycLvV6P5557DkOHDkWvXr0A1Hyv3N3d4e/vb3Jta/9e1VVXADB9+nR06tQJ4eHhOHHiBP76178iKysL33zzjYyllc/JkycRExODyspKeHt7Y9u2bYiKikJ6erqs3ysGGbKbCRMmGH/fp08fDB48GJ06dcK///1vzJkzR8aSUUszbdo04+979+6NPn36IDIyEvv27cPo0aNlLJl85s6di4yMDI5Lk8BSXT311FPG3/fu3RthYWEYPXo0zp8/j8jIyOYupuy6deuG9PR0FBcX4+uvv8asWbOwf/9+uYvFwb7WCgoKglKpNBuNnZeXh9DQUJlK5Rz8/f1x++2349y5c3IXxeEZvkv8njVO586dERQU1Gq/a/PmzcP333+Pn3/+Ge3btzceDw0NRVVVFYqKikyub83fK0t1VZfBgwcDQKv9Xrm7u6NLly4YMGAAli9fjr59++K9996T/XvFIGMld3d3DBgwAHv27DEe0+v12LNnD2JiYmQsmeMrKyvD+fPnERYWJndRHF5ERARCQ0NNvmclJSU4cuQIv2cS/P777ygoKGh13zUhBObNm4dt27Zh7969iIiIMDk/YMAAuLm5mXyvsrKycOnSpVb3vWqoruqSnp4OAK3ue2WJXq+HRqOR/3tl9+HELdBXX30lVCqV2Lhxo8jMzBRPPfWU8Pf3F7m5uXIXzaE8//zzYt++fSI7O1v88ssvIjY2VgQFBYn8/Hy5i+YQSktLxbFjx8SxY8cEAPH222+LY8eOiYsXLwohhFixYoXw9/cX27dvFydOnBD333+/iIiIEDdv3pS55M2vvroqLS0Vf/7zn0VSUpLIzs4WP/30k+jfv7/o2rWrqKyslLvozSouLk74+fmJffv2iZycHOOviooK4zXPPPOM6Nixo9i7d684evSoiImJETExMTKWWh4N1dW5c+fEa6+9Jo4ePSqys7PF9u3bRefOncWIESNkLrk8XnzxRbF//36RnZ0tTpw4IV588UWhUCjE7t27hRDyfq8YZBrpn//8p+jYsaNwd3cX0dHR4vDhw3IXyeE89NBDIiwsTLi7u4t27dqJhx56SJw7d07uYjmMn3/+WQAw+zVr1iwhRM0U7FdeeUWEhIQIlUolRo8eLbKysuQttEzqq6uKigoxduxY0bZtW+Hm5iY6deoknnzyyVb5D4u66giA2LBhg/GamzdvimeffVa0adNGeHp6ij/96U8iJydHvkLLpKG6unTpkhgxYoQICAgQKpVKdOnSRfzlL38RxcXF8hZcJo8//rjo1KmTcHd3F23bthWjR482hhgh5P1eKYQQwv7tPkRERES2xzEyRERE5LQYZIiIiMhpMcgQERGR02KQISIiIqfFIENEREROi0GGiIiInBaDDBERETktBhkiIiJyWgwyRCSbpKQkKJVKTJw4sVme99e//hW33XYbSktLTY5PmjQJI0aMgF6vb5ZyEJHtcGVfIpLNE088AW9vb6xfvx5ZWVkIDw+36/MMG9zFxMRg3bp1AIBPP/0UCxYswPHjxxEZGWnX5xOR7bFFhohkUVZWhi1btiAuLg4TJ07Exo0bTc6/9tprCA8PR0FBgfHYxIkTcffddxtbTg4ePIjhw4fDw8MDHTp0wIIFC1BeXm7xmSqVCp999hk+++wzJCQk4NKlS1i0aBFWrVrFEEPkpNgiQ0Sy+PTTT7FmzRqkpKTg+++/x3PPPYezZ89CoVAAAHQ6HYYPH46QkBBs27YNq1evxssvv4zjx4+jY8eOOH/+PPr27YvXX38dEydOxLVr1zBv3jz07dsXGzZsqPfZS5cuxfr16xEZGQl3d3fs3r3b+Fwici4MMkQki6FDh2Lq1KlYuHAhqqurERYWhq1bt2LkyJHGa3777Tf069cPzz77LN5//3188sknmD59OoCabimlUomPP/7YeP3Bgwdx1113oby8HGq12uKztVotIiMjkZ+fj19//RUdO3a02+ckIvti1xIRNbusrCwkJyfj4YcfBgC4urrioYcewvr1602u69y5M958802sXLkS9913nzHEAMDx48exceNGeHt7G3+NGzcOer0e2dnZ9T4/MTERubm50Ov1SElJsf0HJKJm4yp3AYio9Vm/fj2qq6tNBvcKIaBSqfDBBx/Az8/PePzAgQNQKpW4cOECqqur4epa89dWWVkZnn76aSxYsMDs/vW1sNy4cQNPPvkkXn75ZQgh8Oyzz+Kuu+5CUFCQDT8hETUXdi0RUbOqrq5G+/bt8cILL2Ds2LEm5x544AH8+c9/xjPPPAMA2LJlC2bPno3du3dj6tSpePLJJxEfHw8AmDFjBvLy8vDTTz9Z9fyZM2fi9OnTOHLkCAAgOjoaXbt2xZYtW2zw6Yio2Qkioma0bds24e7uLoqKiszOvfDCC2LgwIFCCCEuX74s2rRpI95//30hhBAJCQnC1dVVJCUlCSGEOH78uPDw8BBz584Vx44dE7/++qv49ttvxdy5cy0++5tvvhHu7u7i5MmTxmMnTpwQ7u7u4uuvv7blxySiZsIWGSJqVpMmTYJer8fOnTvNziUnJ2Pw4MFIT0/H888/D1dXV+zatcs4o2jBggX44YcfkJ6eDm9vb6SkpOBvf/sbkpKSIIRAZGQkHnroIbz00ktm975+/Tp69uyJhQsXmp1/44038N577+HUqVPsYiJyMgwyRERE5LQ4a4mIiIicFoMMEREROS0GGSIiInJaDDJERETktBhkiIiIyGkxyBAREZHTYpAhIiIip8UgQ0RERE6LQYaIiIicFoMMEREROS0GGSIiInJaDDJERETktP4/OVe0BHZ8DqAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modèle rationnel :** \\begin{equation}\n",
        "y = a + \\frac{1}{b x}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "gom4ozIGFdgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.normal(0, 0.05, 50)\n",
        "b = np.random.normal(2, 0.25, 50)\n",
        "\n",
        "# Générer les données\n",
        "x_data = np.linspace(1/2, 5, 50)  # Réduire l'intervalle de x pour éviter des valeurs trop grandes\n",
        "y_data = a + 1/(b*x_data)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(x_data, y_data)\n",
        "\n",
        "# Ajout de titres et de labels\n",
        "plt.title('Nuage de points')\n",
        "plt.xlabel('Axe X')\n",
        "plt.ylabel('Axe Y')\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def modele_exponentiel(coef):\n",
        "  return y_data -(coef[0] +  1/(coef[1] * x_data))\n",
        "\n",
        "j = jacobian(modele_exponentiel)\n",
        "#print(j(np.array([3,0.1])))\n",
        "\n",
        "sol = gauss_newtone(modele_exponentiel, j, np.array([1.0, 3.0]))\n",
        "#print(sol)\n",
        "\n",
        "plt.plot(np.linspace(1/2, 5, 100),sol[0] + 1/(sol[1]*np.linspace(1/2, 5, 100)))\n",
        "# Affichage du nuage de points\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "9TxTCZQ9kAZ4",
        "outputId": "8845a36a-d0c3-4735-89f5-9e9ad0fae27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le nombre d'itération est de: 6\n",
            "norme du gradient: 3.609331107532551e-13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xmwe5EDeChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Méthode du simplexe"
      ],
      "metadata": {
        "id": "MeqMnUU9_hLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode du simplexe est une technique d'optimisation utilisée pour résoudre des problèmes de programmation linéaire. Elle vise à maximiser ou minimiser une fonction objective linéaire sous un ensemble de contraintes linéaires.\n",
        "$$\n",
        "\\text{Maximiser } c^T x \\quad \\text{sous les contraintes} \\quad Ax \\leq b, \\, x \\geq 0\n",
        "$$\n",
        "\n",
        "où $x \\in \\mathbb{R}^n$, $c \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$ et $b \\in \\mathbb{R}^m$.\n",
        "\n",
        "1.\n",
        "La méthode du simplexe, permet de naviguer dans les sommets d'un polytope défini par les contraintes du problème. Elle part d'un sommet de la solution feasible (acceptable) et se déplace le long des arêtes pour atteindre un sommet optimal, en suivant la direction qui améliore la fonction objective.\n",
        "\n",
        "2. **Théorèmes de Base**\n",
        "Théorème de l'existence de la solution optimale : Si un problème de programmation linéaire a une solution optimale, alors cette solution se trouve dans l'ensemble des sommets du polytope de solutions feasibles.\n",
        "Théorème de la dualité : Chaque problème de programmation linéaire (problème primal) a un problème dual associé. Les valeurs optimales du primal et du dual sont égales sous certaines conditions, permettant d'analyser les solutions de manière complémentaire.\n",
        "\n",
        "3. **Critère d'Optimalité**\n",
        "Le critère d'optimalité dans la méthode du simplexe repose sur l'évaluation des coefficients de la fonction objective dans la base actuelle. Si tous les coefficients des variables non basiques sont non positifs (pour un problème de maximisation), la solution est optimale.\n",
        "\n",
        "4. **Fonctionnement de la Méthode**\n",
        "Initialisation : Commence avec une solution de base réalisable.\n",
        "Itérations : À chaque itération, le simplexe évalue les coefficients de la fonction objective. Si la condition d'optimalité n'est pas satisfaite, il choisit la variable à entrer dans la base et détermine la variable à sortir.\n",
        "Convergence : Le processus se répète jusqu'à ce qu'aucune amélioration supplémentaire ne soit possible, atteignant ainsi la solution optimale.\n",
        "\n"
      ],
      "metadata": {
        "id": "q-KiAw4vEsdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class simplexe:\n",
        "  \"\"\"\n",
        "    Classe représentant l'algorithme du simplexe pour résoudre des problèmes d'optimisation linéaire.\n",
        "\n",
        "    :param f: Coefficients de la fonction objectif à maximiser ou minimiser.\n",
        "    :param A: Matrice des coefficients des contraintes (système d'équations linéaires).\n",
        "    :param b: Vecteur des constantes des contraintes (côté droit des équations).\n",
        "\n",
        "    :return: Aucune valeur retournée directement; affiche la solution optimale à la fin de l'optimisation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, f, A, b):\n",
        "    self.n = len(f) #nombre de variable de la fonction\n",
        "\n",
        "    #construction du tableau\n",
        "    matrix = np.array(A,  dtype=float)\n",
        "    matrix = np.column_stack((matrix, b))\n",
        "    c = np.concatenate((f, np.zeros(A.shape[0] + 1)))\n",
        "    self.matrix = np.vstack((c, matrix))\n",
        "\n",
        "\n",
        "    #appelle de la méthode solve pour trouver la solution optimial\n",
        "    self.solve()\n",
        "    self.afficher_sol()\n",
        "\n",
        "\n",
        "  def changement_base(self, row, col):\n",
        "    \"\"\"\n",
        "        Effectue le changement de base dans le tableau du simplexe.\n",
        "\n",
        "        :param row: Index de la ligne de pivot (ligne entrante).\n",
        "        :param col: Index de la colonne de pivot (colonne entrante).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Diviser la ligne de pivot pour rendre matrix[row, col] égal à 1\n",
        "    self.matrix[row] = self.matrix[row] / self.matrix[row, col]\n",
        "\n",
        "    # 2. Soustraire un multiple de la ligne de pivot de chaque autre ligne\n",
        "    for i in range(len(self.matrix)):\n",
        "        if i != row:\n",
        "            # Soustraire un multiple de la ligne de pivot pour obtenir un 0 dans la colonne `col`\n",
        "            self.matrix[i] = self.matrix[i] - self.matrix[i, col] * self.matrix[row]\n",
        "\n",
        "\n",
        "  def solve(self):\n",
        "    \"\"\"\n",
        "        Résout le problème d'optimisation linéaire en utilisant l'algorithme du simplexe.\n",
        "\n",
        "        Cette méthode itère pour trouver la colonne de la variable entrante et détermine la ligne de pivot\n",
        "        correspondante, en effectuant des opérations sur le tableau du simplexe jusqu'à ce qu'une solution optimale soit atteinte.\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "\n",
        "      #trouver la colonne de la variable entrente\n",
        "      col_entrante = np.argmin(self.matrix[0, :-1])\n",
        "\n",
        "      # Si tous les coûts sont positifs, la solution est optimale\n",
        "      if self.matrix[0, col_entrante] >= 0:\n",
        "        break\n",
        "\n",
        "      # Trouver le pivot\n",
        "      ratios = self.matrix[1:, -1] / self.matrix[1:, col_entrante]\n",
        "      ratios[self.matrix[1:, col_entrante] <= 0] = np.inf\n",
        "      row_sortante = np.argmin(ratios) + 1\n",
        "\n",
        "      #changer la base dans le tableau\n",
        "      self.changement_base(row=row_sortante, col=col_entrante)\n",
        "\n",
        "\n",
        "  def afficher_sol(self):\n",
        "    \"\"\"\n",
        "        Affiche la solution optimale trouvée par l'algorithme du simplexe.\n",
        "\n",
        "        La méthode analyse le tableau du simplexe pour extraire les valeurs des variables dans la solution\n",
        "        et les imprime à l'utilisateur.\n",
        "    \"\"\"\n",
        "\n",
        "    vec_can = lambda ligne : np.count_nonzero(ligne == 1) == 1 and np.count_nonzero(ligne != 0) == 1\n",
        "\n",
        "    sol = []\n",
        "    for i in range(self.n):\n",
        "      if vec_can(self.matrix[:, i]):\n",
        "        row = np.where(self.matrix[:, i] == 1)[0]\n",
        "        sol.append(self.matrix[row[0], -1])\n",
        "\n",
        "    sol += [0] * (self.n - len(sol))\n",
        "    print(f\"La solution est: {sol}\")"
      ],
      "metadata": {
        "id": "zF-RB0H4um1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemple**"
      ],
      "metadata": {
        "id": "l-VqxsStIGXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un gestionnaire de portefeuille d'obligations a 100 000 \\$ à allouer à deux obligations différentes : une obligation d'entreprise et une obligation d'État. L'obligation d'entreprise a un rendement de 4\\%, une maturité de 3 ans et une note A d'une agence de notation, qui est traduite par une note numérique de 2 à des fins de calcul. En revanche, l'obligation d'État a un rendement de 3\\%, une maturité de 4 ans et une note Aaa avec une note numérique correspondante de 1 (des notes numériques plus faibles correspondent à des obligations de meilleure qualité). Le gestionnaire de portefeuille souhaite allouer ses fonds de manière à ce que la note moyenne du portefeuille ne soit pas inférieure à Aa (équivalent numérique 1,5) et que la maturité moyenne du portefeuille ne dépasse pas 3,6 ans. Tout montant non investi dans les deux obligations sera conservé dans un compte de liquidités qui, pour simplifier, est supposé ne rapporter aucun intérêt et ne contribue pas au calcul de la note ou de la maturité moyenne.\n",
        "\n",
        "\n",
        "En notant $ x_1$ et $ x_2 $ comme l'allocation de fonds à l'obligation d'entreprise et à l'obligation d'État respectivement (en milliers de dollars), nous obtenons la formulation suivante pour le problème du gestionnaire de portefeuille :\n",
        "\n",
        "$$\n",
        "\\max 4x_1 + 3x_2\n",
        "$$\n",
        "\n",
        "sous les contraintes :\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "x_1 + x_2 & \\leq 100 \\\\\n",
        "\\frac {2x_1 + x_2}{100} & \\leq 1.5 \\\\\n",
        "\\frac{3x_1 + 4x_2}{100}& \\leq 3.6 \\\\\n",
        "x_1, x_2 & \\geq 0.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Nous multiplions d'abord les deux dernières inégalités par 100 pour éviter les fractions. Après avoir ajouté des variables d'écart à chacune des contraintes fonctionnelles, nous obtenons une représentation du problème sous forme standard, adaptée à la méthode du simplexe. Par exemple, en notant $ x_3 $ le montant que nous gardons en liquidités, nous pouvons réécrire la première contrainte comme $x_1 + x_2 + x_3 = 100 $ avec la condition supplémentaire de $ x_3 \\geq 0 $. En poursuivant cette stratégie, nous obtenons la formulation suivante :\n",
        "\n",
        "$$\n",
        "\\max  4x_1 + 3x_2\n",
        "$$\n",
        "\n",
        "sous les contraintes :\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "x_1 + x_2 + x_3 & = 100 \\\\\n",
        "2x_1 + x_2 + x_4 & = 150 \\\\\n",
        "3x_1 + 4x_2 + x_5 & = 360 \\\\\n",
        "x_1, x_2, x_3, x_4, x_5 & \\geq 0.\n",
        "\\end{align*}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "aJ8SOOWtIamx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rendement = np.array([-4, -3])  # Nous utilisons des coefficients négatifs pour la maximisation\n",
        "\n",
        "# Matrice des coefficients des contraintes\n",
        "A = np.array([\n",
        "    [1, 1, 1, 0, 0],   # x1 + x2 + x3 = 100\n",
        "    [2, 1, 0, 1, 0],   # 2x1 + x2 + x4 = 150\n",
        "    [3, 4, 0, 0, 1]    # 3x1 + 4x2 + x5 = 360\n",
        "])\n",
        "\n",
        "# Vecteur des limites des contraintes\n",
        "b = np.array([100, 150, 360])\n",
        "\n",
        "allocation = simplexe(rendement, A, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWXV0de2IInw",
        "outputId": "30419312-6723-4dd1-d714-4ea846c61af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La solution est: [50.0, 50.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'allocation optimale pour maximiser le rendement du portefeuille en respectant les contraintes serait de 50 % dans l'obligation d'État et de 50 % dans l'obligation gouvernementale. Ce résultat a été validé en utilisant la méthode de programmation linéaire de SciPy, et il correspond à l'exemple présenté dans le livre Optimization Methods in Finance de Gérard Cornuejols et Reha Tutuncu.\n",
        "\n"
      ],
      "metadata": {
        "id": "XrODbtbpNHHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interior-Point Methods"
      ],
      "metadata": {
        "id": "UwanCLXISpFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def interior_point(c, A, b, sigma=0.1, alpha=0.9, x_k=None, lambda_k=None, s_k=None):\n",
        "  n = A.shape[1]  # nombre de variables\n",
        "  m = A.shape[0]\n",
        "\n",
        "  if x_k is None and lambda_k is None and s_k is None:\n",
        "    x_k = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n",
        "    lambda_k = np.zeros(m)\n",
        "    s_k = np.ones(n) * 1/2\n",
        "\n",
        "\n",
        "  F = lambda x_k_F, lambda_k_F, s_k_F: np.block([\n",
        "        A.T @ lambda_k_F + s_k_F - c,  # doit avoir la même dimension que c\n",
        "        A @ x_k_F - b,                  # dimension doit être m\n",
        "        s_k_F * x_k_F    # produit de deux vecteurs # s_k_F @ x_k_F * np.ones(n)\n",
        "    ])\n",
        "\n",
        "\n",
        "  jacobien = lambda x_k_j, s_k_j: np.block([\n",
        "        [np.zeros((n, n)), A.T, np.eye(n)],       # Bloc de taille (n, n+m+n)\n",
        "        [A, np.zeros((m, m)), np.zeros((m, n))],  # Bloc de taille (m, n+m+n)\n",
        "        [np.diag(s_k_j), np.zeros((n, m)), np.diag(x_k_j)]  # Bloc de taille (n, n+m+n)\n",
        "    ])\n",
        "\n",
        "  print(x_k.shape)\n",
        "  print(A.shape)\n",
        "  dim_F = len(F(x_k, lambda_k, s_k))\n",
        "  mu_k = lambda x_k_mu, s_k_mu: np.append(np.zeros(dim_F - n), sigma * np.dot(x_k_mu, s_k_mu) / n * np.ones(n))\n",
        "  print(mu_k(x_k,s_k))\n",
        "  i = 0\n",
        "  while np.linalg.norm(F(x_k, lambda_k, s_k)) >  0.1 and i < 10:\n",
        "    print(jacobien(x_k, s_k))\n",
        "\n",
        "    sol_sys_lin = np.linalg.solve(jacobien(x_k, s_k), -F(x_k, lambda_k, s_k) + mu_k(x_k, s_k))\n",
        "    # while np.dot(x_k,s_k)<0 :\n",
        "    # alpha /= 2\n",
        "    sol_sys_lin += alpha*sol_sys_lin\n",
        "    x_k = sol_sys_lin[:n]\n",
        "    lambda_k = sol_sys_lin[n:n+m]\n",
        "    s_k = sol_sys_lin[n+m: 2*n + m]\n",
        "    #break\n",
        "    i+=1\n",
        "\n",
        "  return x_k\n",
        "\n",
        "\n",
        "# f = np.array([1, 1])  # Nous utilisons des coefficients négatifs pour la maximisation\n",
        "\n",
        "# # Matrice des coefficients des contraintes\n",
        "# A = np.array([\n",
        "#      [1, 2],\n",
        "#      [2, -1]\n",
        "# ])\n",
        "\n",
        "# # Vecteur des limites des contraintes\n",
        "# b = np.array([1, 0])\n",
        "\n",
        "# print(interior_point(f, A, b))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sOVzRNrrSwLi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "288f536e-5806-45a7-b299-544d4d6ea71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef interior_point(c, A, b, sigma=0.1, alpha=0.9, x_k=None, lambda_k=None, s_k=None):\\n  n = A.shape[1]  # nombre de variables\\n  m = A.shape[0]\\n\\n  if x_k is None and lambda_k is None and s_k is None:\\n    x_k = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\\n    lambda_k = np.zeros(m)\\n    s_k = np.ones(n) * 1/2\\n\\n\\n  F = lambda x_k_F, lambda_k_F, s_k_F: np.block([\\n        A.T @ lambda_k_F + s_k_F - c,  # doit avoir la même dimension que c\\n        A @ x_k_F - b,                  # dimension doit être m\\n        s_k_F * x_k_F    # produit de deux vecteurs # s_k_F @ x_k_F * np.ones(n)\\n    ])\\n\\n\\n  jacobien = lambda x_k_j, s_k_j: np.block([\\n        [np.zeros((n, n)), A.T, np.eye(n)],       # Bloc de taille (n, n+m+n)\\n        [A, np.zeros((m, m)), np.zeros((m, n))],  # Bloc de taille (m, n+m+n)\\n        [np.diag(s_k_j), np.zeros((n, m)), np.diag(x_k_j)]  # Bloc de taille (n, n+m+n)\\n    ])\\n\\n  print(x_k.shape)\\n  print(A.shape)\\n  dim_F = len(F(x_k, lambda_k, s_k))\\n  mu_k = lambda x_k_mu, s_k_mu: np.append(np.zeros(dim_F - n), sigma * np.dot(x_k_mu, s_k_mu) / n * np.ones(n))\\n  print(mu_k(x_k,s_k))\\n  i = 0\\n  while np.linalg.norm(F(x_k, lambda_k, s_k)) >  0.1 and i < 10:\\n    print(jacobien(x_k, s_k))\\n\\n    sol_sys_lin = np.linalg.solve(jacobien(x_k, s_k), -F(x_k, lambda_k, s_k) + mu_k(x_k, s_k))\\n    # while np.dot(x_k,s_k)<0 :\\n    # alpha /= 2\\n    sol_sys_lin += alpha*sol_sys_lin\\n    x_k = sol_sys_lin[:n]\\n    lambda_k = sol_sys_lin[n:n+m]\\n    s_k = sol_sys_lin[n+m: 2*n + m]\\n    #break\\n    i+=1\\n\\n  return x_k\\n\\n\\n# f = np.array([1, 1])  # Nous utilisons des coefficients négatifs pour la maximisation\\n\\n# # Matrice des coefficients des contraintes\\n# A = np.array([\\n#      [1, 2],\\n#      [2, -1]\\n# ])\\n\\n# # Vecteur des limites des contraintes\\n# b = np.array([1, 0])\\n\\n# print(interior_point(f, A, b))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzQfqvQ6I67v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QP"
      ],
      "metadata": {
        "id": "C4f02Cmle5Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "from scipy.linalg import pinv"
      ],
      "metadata": {
        "id": "Iodwh3aFe9-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def active_set_method(G, c, A, b, x_0):\n",
        "  #dimension matrice inegalité\n",
        "  n_row, n_col = A.shape\n",
        "\n",
        "  #trouver le premier working set\n",
        "  W = []\n",
        "  for i in range(n_row):\n",
        "    if A[i, :] @ x_0 - b[i] == 0:\n",
        "      #contrainte active\n",
        "      W.append(i)\n",
        "\n",
        "\n",
        "  #probleme de minimisation contraite d'égalité pour trouver la direction de la prochaine iteration\n",
        "  def objective(p):\n",
        "    return 0.5 * p.T @ G @ p + (G @ x_0 + c).T @ p\n",
        "\n",
        "  # Fonction contrainte\n",
        "  def constraint():\n",
        "    # Trier W pour garantir l'ordre croissant des lignes de A\n",
        "    W_sorted = sorted(W)\n",
        "    mat = np.vstack([A[i, :] for i in W_sorted])\n",
        "    return mat\n",
        "\n",
        "  #step length\n",
        "  def alpha():\n",
        "    inactive_indices = [i for i in range(len(b)) if i not in W]\n",
        "    alpha_k = float('inf')\n",
        "\n",
        "    idx = 0\n",
        "    # Calculer alpha_k\n",
        "    for i in inactive_indices:\n",
        "        a_i = A[i]  # a_i est maintenant une seule ligne de A\n",
        "        if np.dot(a_i, p_k) < 0:  # Vérifier si le produit scalaire est négatif\n",
        "            alpha_candidate = (b[i] - np.dot(a_i.T, x_0)) / np.dot(a_i.T, p_k)\n",
        "            if alpha_candidate < alpha_k:\n",
        "                idx = i\n",
        "                alpha_k = alpha_candidate\n",
        "\n",
        "    if alpha_k < 1:\n",
        "      W.append(idx)\n",
        "      return alpha_k\n",
        "    else:\n",
        "      return 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  iter = 0\n",
        "  while iter < 1000:\n",
        "    #1) Trouver la direction p_k\n",
        "    if len(W) > 0:\n",
        "      const = constraint()\n",
        "      sol_p = minimize(objective, np.zeros(n_col), method='SLSQP', constraints={'type': 'eq', 'fun': lambda p: const @ p})\n",
        "      p_k = sol_p.x\n",
        "\n",
        "    else: #le working set est vide donc optimisation sans contrainte\n",
        "       sol_p = minimize(objective, np.zeros(n_col))  # Minimisation sans contraintes\n",
        "       if sol_p.success:\n",
        "         p_k = sol_p.x\n",
        "\n",
        "\n",
        "    #apres avoir trouver p_k verifier si il est different de 0\n",
        "    if  np.all(p_k == 0):\n",
        "      #trouver les lambdas\n",
        "      lambda_ = pinv(constraint().T) @ (G @ x_0 + c)\n",
        "      if np.all(lambda_ >= 0):\n",
        "        return x_0\n",
        "\n",
        "      else:#on retire un indice du working set\n",
        "        W = sorted(W)\n",
        "        j = np.argmin(lambda_)\n",
        "        W.pop(j)\n",
        "\n",
        "\n",
        "    else: #si pk different de 0\n",
        "       #trouver un step alpha\n",
        "       alpha_k = alpha()\n",
        "       x_0 += alpha_k * p_k\n",
        "\n",
        "\n",
        "    iter += 1"
      ],
      "metadata": {
        "id": "VU3UDvdyV1QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = np.array([[2, 0], [0, 2]])\n",
        "c =  np.array([-2, -5])\n",
        "\n",
        "    # Contraintes\n",
        "A = np.array([\n",
        "        [1, -2],  # x1 - 2x2 + 2 >= 0\n",
        "        [-1, -2],  # -x1 - 2x2 + 6 >= 0\n",
        "        [-1, 2],  # -x1 + 2x2 + 2 >= 0\n",
        "        [1, 0],   # x1 >= 0\n",
        "        [0, 1]    # x2 >= 0\n",
        "    ])\n",
        "b = np.array([-2, -6, -2, 0, 0])\n",
        "\n",
        "    # Point initial\n",
        "x_0 = np.array([2.0, 0.0])\n",
        "\n",
        "    # Appel de la méthode\n",
        "active_set_method(G, c, A, b, x_0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwcZ0sXRXeFK",
        "outputId": "adbe0c36-24dd-4e5c-bee3-8949ed6cd0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.4, 1.7])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhKUckxlV170"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penalty method\n"
      ],
      "metadata": {
        "id": "6vVuLwQZVLyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def penalty_function(x, f, constraints, mu):\n",
        "    \"\"\"\n",
        "    Fonction pénalisée quadratique pour l'optimisation contrainte.\n",
        "\n",
        "    Arguments:\n",
        "    - x : variables de décision\n",
        "    - f : fonction objectif sans contraintes\n",
        "    - constraints : liste de contraintes de la forme [c1, c2, ...], où chaque ci est une fonction qui représente une contrainte d'égalité ou d'inégalité.\n",
        "    - mu : paramètre de pénalité\n",
        "\n",
        "    Retourne la valeur de la fonction pénalisée Q(x; mu).\n",
        "    \"\"\"\n",
        "    penalty = 0\n",
        "    for c in constraints:\n",
        "        penalty += mu * max(0, c(x))**2\n",
        "    return f(x) + penalty\n",
        "\n",
        "def quadratic_penalty_method(f, constraints, x0, mu0=1.0, tau0=1e-8, beta=10, tol=1e-6, max_iter=100):\n",
        "    \"\"\"\n",
        "    Méthode de pénalité quadratique.\n",
        "\n",
        "    Arguments:\n",
        "    - f : fonction objectif à minimiser (sans contraintes)\n",
        "    - constraints : liste de contraintes sous forme de fonctions [c1, c2, ...]\n",
        "    - x0 : point de départ\n",
        "    - mu0 : valeur initiale du paramètre de pénalité (par défaut 1.0)\n",
        "    - tau0 : tolérance initiale pour la norme du gradient (par défaut 1e-2)\n",
        "    - beta : facteur de croissance du paramètre de pénalité mu (par défaut 10)\n",
        "    - tol : tolérance pour l'arrêt global (par défaut 1e-6)\n",
        "    - max_iter : nombre maximal d'itérations\n",
        "\n",
        "    Retourne la solution optimale approximative x.\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    mu = mu0\n",
        "    tau = tau0\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        # Définir la fonction pénalisée Q(x; mu)\n",
        "        Q = lambda x: penalty_function(x, f, constraints, mu)\n",
        "\n",
        "        # Minimiser Q(x; mu) à partir de x avec tolérance de gradient tau\n",
        "        result = minimize(Q, x, method='BFGS', tol=tau)\n",
        "        x = result.x\n",
        "\n",
        "        # Vérifier la condition de convergence\n",
        "        if np.linalg.norm(result.jac) < tol:\n",
        "            print(f\"Convergence atteinte à l'itération {k}.\")\n",
        "            break\n",
        "\n",
        "        # Mettre à jour le paramètre de pénalité et la tolérance\n",
        "        mu *= beta  # augmenter la pénalité\n",
        "        tau /= 2    # diminuer la tolérance\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "f = lambda x: (x[0] - 1)**2 + (x[1] - 2)**2\n",
        "\n",
        "\n",
        "constraints = [\n",
        "    lambda x: x[0] + x[1] - 3,\n",
        "    lambda x: -(x[0] - 1)\n",
        "]\n",
        "\n",
        "# Point de départ\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "# Appel de l'algorithme de pénalité quadratique\n",
        "solution = quadratic_penalty_method(f, constraints, x0)\n",
        "\n",
        "print(\"Solution trouvée :\", solution)"
      ],
      "metadata": {
        "id": "AmPlVfIJXQxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97e3cd3-c66a-47c2-afb0-2d17a6065226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence atteinte à l'itération 0.\n",
            "Solution trouvée : [0.99999999 1.99999999]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9ldtd5SAfejo",
        "SaS78X5YOU1j",
        "Ak91EktIgMK1",
        "4pYgxl1qiCKs",
        "MeqMnUU9_hLl",
        "UwanCLXISpFd",
        "C4f02Cmle5Zl",
        "6vVuLwQZVLyI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}